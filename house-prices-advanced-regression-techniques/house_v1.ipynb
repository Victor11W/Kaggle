{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1edf55c",
   "metadata": {},
   "source": [
    "# House "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e364d37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import os\n",
    "\n",
    "# Désactive les erreurs Ray parasites\n",
    "os.environ[\"RAY_IGNORE_UNHANDLED_ERRORS\"] = \"1\"\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "test_ids = test['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23c5aea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'LotShape',\n",
      "       'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1',\n",
      "       'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'RoofStyle',\n",
      "       'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'ExterQual',\n",
      "       'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
      "       'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
      "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
      "       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
      "       'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n",
      "       'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageCars',\n",
      "       'GarageQual', 'GarageCond', 'PavedDrive', 'MoSold', 'YrSold',\n",
      "       'SaleType', 'SaleCondition', 'Age_Built', 'Age_RemodAdd',\n",
      "       'HasBsmtFinSF2', 'HasBsmtFinSF1', 'HeatingQC_num', 'HasLowQualFinSF',\n",
      "       'GarageAge', 'WoodDeckSF_log', 'OpenPorchSF_log', 'HasScreenPorch',\n",
      "       'ScreenPorchLog', 'GotPool', 'Fence_wo', 'Fence_Prv', 'MoSold_sin',\n",
      "       'MoSold_cos', 'TotalArea', 'BathPerRoom', 'OverallQual_GrLiv',\n",
      "       'Qual_x_Bath', 'Neighborhood_mean'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "y= train[\"SalePrice\"]\n",
    "train = train.drop(columns=[\"SalePrice\",\"Id\"])\n",
    "test = test.drop(columns=[\"Id\"])\n",
    "\n",
    "# MSZoning - replace NaN with mode (RL)\n",
    "train[\"MSZoning\"] = train[\"MSZoning\"].replace(np.nan, \"RL\")\n",
    "test[\"MSZoning\"] = test[\"MSZoning\"].replace(np.nan, \"RL\")\n",
    "\n",
    "\n",
    "# LotFrontage - replace NaN with median of neighborhood + log transform\n",
    "train[\"LotFrontage\"] = train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "test[\"LotFrontage\"] = test.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "train[\"LotFrontage\"] = np.log1p(train[\"LotFrontage\"])  # log transform for skew\n",
    "test[\"LotFrontage\"] = np.log1p(test[\"LotFrontage\"])  # log transform for skew\n",
    "\n",
    "# Age_Built and Age_RemodAdd - create new features\n",
    "train[\"Age_Built\"] = train[\"YrSold\"] - train[\"YearBuilt\"]\n",
    "train[\"Age_RemodAdd\"] = train[\"YrSold\"] - train[\"YearRemodAdd\"]\n",
    "test[\"Age_Built\"] = test[\"YrSold\"] - test[\"YearBuilt\"]\n",
    "test[\"Age_RemodAdd\"] = test[\"YrSold\"] - test[\"YearRemodAdd\"]\n",
    "\n",
    "# LotArea - use log transformation to reduce skewness\n",
    "train[\"LotArea\"] = np.log1p(train[\"LotArea\"])\n",
    "test[\"LotArea\"] = np.log1p(test[\"LotArea\"])\n",
    "\n",
    "\n",
    "\n",
    "# MasVnrType - replace NaN with None\n",
    "train[\"MasVnrType\"] = train[\"MasVnrType\"].replace(np.nan, \"None\")\n",
    "test[\"MasVnrType\"] = test[\"MasVnrType\"].replace(np.nan, \"None\")\n",
    "\n",
    "# MasVnrArea - replace NaN with 0 + log transform\n",
    "train[\"MasVnrArea\"] = train[\"MasVnrArea\"].replace(np.nan, 0)\n",
    "test[\"MasVnrArea\"] = test[\"MasVnrArea\"].replace(np.nan, 0)\n",
    "train[\"MasVnrArea\"] = np.log1p(train[\"MasVnrArea\"])  # log transform for skew\n",
    "test[\"MasVnrArea\"] = np.log1p(test[\"MasVnrArea\"])  # log transform for skew\n",
    "\n",
    "# ExterCond - mapping\n",
    "mapping = {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "train[\"ExterCond\"] = train[\"ExterCond\"].map(mapping)\n",
    "test[\"ExterCond\"] = test[\"ExterCond\"].map(mapping)\n",
    "# ExterQual - mapping\n",
    "mapping = {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "train[\"ExterQual\"] = train[\"ExterQual\"].map(mapping)\n",
    "test[\"ExterQual\"] = test[\"ExterQual\"].map(mapping)\n",
    "\n",
    "#BstmQual - replace NaN with None + mapping\n",
    "train[\"BsmtQual\"] = train[\"BsmtQual\"].replace(np.nan, \"None\")\n",
    "test[\"BsmtQual\"] = test[\"BsmtQual\"].replace(np.nan, \"None\")\n",
    "mapping = {\"None\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "train[\"BsmtQual\"] = train[\"BsmtQual\"].map(mapping)\n",
    "test[\"BsmtQual\"] = test[\"BsmtQual\"].map(mapping)\n",
    "\n",
    "# BsmtCOnd - replace NaN with None + mapping\n",
    "train[\"BsmtCond\"] = train[\"BsmtCond\"].replace(np.nan, \"None\")\n",
    "test[\"BsmtCond\"] = test[\"BsmtCond\"].replace(np.nan, \"None\")\n",
    "mapping = {\"None\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "train[\"BsmtCond\"] = train[\"BsmtCond\"].map(mapping)\n",
    "test[\"BsmtCond\"] = test[\"BsmtCond\"].map(mapping)\n",
    "\n",
    "# BsmtHalfBath - replace NaN with 0\n",
    "train[\"BsmtHalfBath\"] = train[\"BsmtHalfBath\"].replace(np.nan, 0)\n",
    "test[\"BsmtHalfBath\"] = test[\"BsmtHalfBath\"].replace(np.nan, 0)\n",
    "\n",
    "# BsmtFullBath - replace NaN with 0\n",
    "train[\"BsmtFullBath\"] = train[\"BsmtFullBath\"].replace(np.nan, 0)\n",
    "test[\"BsmtFullBath\"] = test[\"BsmtFullBath\"].replace(np.nan, 0)\n",
    "\n",
    "# BsmtExposure - replace NaN with None + mapping\n",
    "train[\"BsmtExposure\"] = train[\"BsmtExposure\"].replace(np.nan, \"None\")\n",
    "test[\"BsmtExposure\"] = test[\"BsmtExposure\"].replace(np.nan, \"None\")\n",
    "mapping = {\"None\": 0, \"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4}\n",
    "train[\"BsmtExposure\"] = train[\"BsmtExposure\"].map(mapping)\n",
    "test[\"BsmtExposure\"] = test[\"BsmtExposure\"].map(mapping)\n",
    "\n",
    "#BsmtfinType1 - replace NaN with None + mapping\n",
    "train[\"BsmtFinType1\"] = train[\"BsmtFinType1\"].replace(np.nan, \"None\")\n",
    "test[\"BsmtFinType1\"] = test[\"BsmtFinType1\"].replace(np.nan, \"None\")\n",
    "mapping = {\"None\": 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\n",
    "train[\"BsmtFinType1\"] = train[\"BsmtFinType1\"].map(mapping)\n",
    "test[\"BsmtFinType1\"] = test[\"BsmtFinType1\"].map(mapping)\n",
    "\n",
    "# BsmtFinSF2 - replace NaN with 0 + log transform + binary indicator\n",
    "train[\"BsmtFinSF2\"] = train[\"BsmtFinSF2\"].replace(np.nan, 0)\n",
    "test[\"BsmtFinSF2\"] = test[\"BsmtFinSF2\"].replace(np.nan, 0)\n",
    "train[\"HasBsmtFinSF2\"] = (train[\"BsmtFinSF2\"] > 0).astype(int)\n",
    "test[\"HasBsmtFinSF2\"] = (test[\"BsmtFinSF2\"] > 0).astype(int)\n",
    "train[\"BsmtFinSF2\"] = np.log1p(train[\"BsmtFinSF2\"])  # log transform for skew\n",
    "test[\"BsmtFinSF2\"] = np.log1p(test[\"BsmtFinSF2\"])  # log transform for skew\n",
    "\n",
    "# BsmtFinSF1 - replace NaN with 0 + log transform + binary indicator\n",
    "train[\"BsmtFinSF1\"] = train[\"BsmtFinSF1\"].replace(np.nan, 0)\n",
    "test[\"BsmtFinSF1\"] = test[\"BsmtFinSF1\"].replace(np.nan, 0)\n",
    "train[\"HasBsmtFinSF1\"] = (train[\"BsmtFinSF1\"] > 0).astype(int)\n",
    "test[\"HasBsmtFinSF1\"] = (test[\"BsmtFinSF1\"] > 0).astype(int)\n",
    "train[\"BsmtFinSF1\"] = np.log1p(train[\"BsmtFinSF1\"])  # log transform for skew\n",
    "test[\"BsmtFinSF1\"] = np.log1p(test[\"BsmtFinSF1\"])  # log transform for skew\n",
    "\n",
    "# BsmtFinType2 - replace NaN with None + mapping\n",
    "train[\"BsmtFinType2\"] = train[\"BsmtFinType2\"].replace(np.nan, \"None\")\n",
    "test[\"BsmtFinType2\"] = test[\"BsmtFinType2\"].replace(np.nan, \"None\")\n",
    "mapping = {\"None\": 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\n",
    "train[\"BsmtFinType2\"] = train[\"BsmtFinType2\"].map(mapping)\n",
    "test[\"BsmtFinType2\"] = test[\"BsmtFinType2\"].map(mapping)\n",
    "\n",
    "# BsmtUnfSF - replace NaN with 0 + log transform\n",
    "train[\"BsmtUnfSF\"] = train[\"BsmtUnfSF\"].replace(np.nan, 0)\n",
    "test[\"BsmtUnfSF\"] = test[\"BsmtUnfSF\"].replace(np.nan, 0)\n",
    "train[\"BsmtUnfSF\"] = np.log1p(train[\"BsmtUnfSF\"])  # log transform for skew\n",
    "test[\"BsmtUnfSF\"] = np.log1p(test[\"BsmtUnfSF\"])  # log transform for skew\n",
    "\n",
    "# TotalBsmtSF - use log transformation to reduce skewness\n",
    "# replace NaN with 0 (no basement)\n",
    "train[\"TotalBsmtSF\"] = train[\"TotalBsmtSF\"].replace(np.nan, 0)\n",
    "test[\"TotalBsmtSF\"] = test[\"TotalBsmtSF\"].replace(np.nan, 0)\n",
    "train[\"TotalBsmtSF\"] = np.log1p(train[\"TotalBsmtSF\"])\n",
    "test[\"TotalBsmtSF\"] = np.log1p(test[\"TotalBsmtSF\"])\n",
    "\n",
    "# HeatingQC \n",
    "mapping = {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "train[\"HeatingQC_num\"] = train[\"HeatingQC\"].map(mapping)\n",
    "test[\"HeatingQC_num\"] = test[\"HeatingQC\"].map(mapping)\n",
    "\n",
    "# Electrical - replace NaN with mode (SBrkr)\n",
    "train[\"Electrical\"] = train[\"Electrical\"].replace(np.nan, \"SBrkr\")\n",
    "test[\"Electrical\"] = test[\"Electrical\"].replace(np.nan, \"SBrkr\")\n",
    "\n",
    "# Exterior1st - replace NaN with mode (VinylSd)\n",
    "train[\"Exterior1st\"] = train[\"Exterior1st\"].replace(np.nan, \"VinylSd\")\n",
    "test[\"Exterior1st\"] = test[\"Exterior1st\"].replace(np.nan, \"VinylSd\")\n",
    "\n",
    "# Exterior2nd - replace NaN with mode (VinylSd)\n",
    "train[\"Exterior2nd\"] = train[\"Exterior2nd\"].replace(np.nan, \"VinylSd\")\n",
    "test[\"Exterior2nd\"] = test[\"Exterior2nd\"].replace(np.nan, \"VinylSd\")\n",
    "\n",
    "# First Floor SF - use log transformation to reduce skewness\n",
    "train[\"1stFlrSF\"] = np.log1p(train[\"1stFlrSF\"])\n",
    "test[\"1stFlrSF\"] = np.log1p(test[\"1stFlrSF\"])\n",
    "\n",
    "# 2nd Flr SF - create binary indicator + log transform\n",
    "train[\"2ndFlrSF\"] = np.log1p(train[\"2ndFlrSF\"])  # log transform for skew\n",
    "test[\"2ndFlrSF\"] = np.log1p(test[\"2ndFlrSF\"])  # log transform for skew\n",
    "\n",
    "# Low Qualfin SF - binomial indicator\n",
    "train[\"HasLowQualFinSF\"] = (train[\"LowQualFinSF\"] > 0).astype(int)\n",
    "test[\"HasLowQualFinSF\"] = (test[\"LowQualFinSF\"] > 0).astype(int)\n",
    "\n",
    "# GrLivArea - use log transformation to reduce skewness\n",
    "train[\"GrLivArea\"] = np.log1p(train[\"GrLivArea\"])\n",
    "test[\"GrLivArea\"] = np.log1p(test[\"GrLivArea\"])\n",
    "\n",
    "# Kitchen Qual - replace NaN with mode (TA) + mapping\n",
    "mapping = {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "train[\"KitchenQual\"] = train[\"KitchenQual\"].replace(np.nan, \"TA\")\n",
    "test[\"KitchenQual\"] = test[\"KitchenQual\"].replace(np.nan, \"TA\")\n",
    "train[\"KitchenQual\"] = train[\"KitchenQual\"].map(mapping)\n",
    "test[\"KitchenQual\"] = test[\"KitchenQual\"].map(mapping)\n",
    "\n",
    "#FireplaceQu - replace NaN with None + mapping\n",
    "mapping = {\"None\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}  \n",
    "train[\"FireplaceQu\"] = train[\"FireplaceQu\"].replace(np.nan, \"None\")\n",
    "test[\"FireplaceQu\"] = test[\"FireplaceQu\"].replace(np.nan, \"None\")\n",
    "train[\"FireplaceQu\"] = train[\"FireplaceQu\"].map(mapping)\n",
    "test[\"FireplaceQu\"] = test[\"FireplaceQu\"].map(mapping)\n",
    "\n",
    "# GarageYrBlt - create new feature GarageAge + replace NaN with maxvalue \n",
    "max_year = max(train[\"YearBuilt\"].max(), train[\"YearRemodAdd\"].max(), train[\"YrSold\"].max()) + 1\n",
    "train[\"GarageYrBlt\"] = train[\"GarageYrBlt\"].replace(np.nan, max_year)\n",
    "test[\"GarageYrBlt\"] = test[\"GarageYrBlt\"].replace(np.nan, max_year)\n",
    "train[\"GarageAge\"] = train[\"YrSold\"] - train[\"GarageYrBlt\"]\n",
    "test[\"GarageAge\"] = test[\"YrSold\"] - test[\"GarageYrBlt\"]\n",
    "\n",
    "# GarageFinish - replace NaN with None + mapping\n",
    "train[\"GarageFinish\"] = train[\"GarageFinish\"].replace(np.nan, \"None\")\n",
    "test[\"GarageFinish\"] = test[\"GarageFinish\"].replace(np.nan, \"None\")\n",
    "mapping = {\"Fin\": 1, \"RFn\": 2, \"Unf\": 3, \"None\": 0}\n",
    "train[\"GarageFinish\"] = train[\"GarageFinish\"].map(mapping)\n",
    "test[\"GarageFinish\"] = test[\"GarageFinish\"].map(mapping)\n",
    "\n",
    "# GarageCond - mapping + replace NaN with None\n",
    "mapping = {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5, \"None\": 0}\n",
    "train[\"GarageCond\"] = train[\"GarageCond\"].replace(np.nan, \"None\")\n",
    "test[\"GarageCond\"] = test[\"GarageCond\"].replace(np.nan, \"None\")\n",
    "train[\"GarageCond\"] = train[\"GarageCond\"].map(mapping)\n",
    "test[\"GarageCond\"] = test[\"GarageCond\"].map(mapping)\n",
    "\n",
    "# GarageCars - create binary indicator and log transform + replace NaN with 0\n",
    "train[\"GarageCars\"] = train[\"GarageCars\"].fillna(0)\n",
    "test[\"GarageCars\"] = test[\"GarageCars\"].fillna(0)\n",
    "\n",
    "\n",
    "# GarageType - replace NaN with None\n",
    "train[\"GarageType\"] = train[\"GarageType\"].replace(np.nan, \"None\")\n",
    "test[\"GarageType\"] = test[\"GarageType\"].replace(np.nan, \"None\")\n",
    "\n",
    "# GarageQual - replace NaN with None + mapping\n",
    "mapping = {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5, \"None\": 0}\n",
    "train[\"GarageQual\"] = train[\"GarageQual\"].replace(np.nan, \"None\")\n",
    "test[\"GarageQual\"] = test[\"GarageQual\"].replace(np.nan, \"None\")\n",
    "train[\"GarageQual\"] = train[\"GarageQual\"].map(mapping)\n",
    "test[\"GarageQual\"] = test[\"GarageQual\"].map(mapping)\n",
    "\n",
    "#Same with WoodDeckSF\n",
    "train[\"WoodDeckSF_log\"] = np.log1p(train[\"WoodDeckSF\"])  # log transform for skew\n",
    "test[\"WoodDeckSF_log\"] = np.log1p(test[\"WoodDeckSF\"])\n",
    "\n",
    "# Optionally keep raw or transform it\n",
    "train[\"OpenPorchSF_log\"] = np.log1p(train[\"OpenPorchSF\"])  # log transform for skew\n",
    "test[\"OpenPorchSF_log\"] = np.log1p(test[\"OpenPorchSF\"])\n",
    "\n",
    "train[\"HasScreenPorch\"] = (train[\"ScreenPorch\"] > 0).astype(int)\n",
    "train[\"ScreenPorchLog\"] = np.log1p(train[\"ScreenPorch\"])\n",
    "test[\"HasScreenPorch\"] = (test[\"ScreenPorch\"] > 0).astype(int)\n",
    "test[\"ScreenPorchLog\"] = np.log1p(test[\"ScreenPorch\"])\n",
    "\n",
    "train[\"GotPool\"] = train[\"PoolQC\"].notnull().astype(int)\n",
    "test[\"GotPool\"] = test[\"PoolQC\"].notnull().astype(int)\n",
    "\n",
    "\n",
    "# Fence - replace NaN with None + mapping with quality and good wood\n",
    "# Fence - mapping with quality and good wood\n",
    "# Fence_wo: 2 if GdWo, 1 if MnWw, else 0\n",
    "# Fence_Prv: 2 if GdPrv, 1 if MnPrv, else 0\n",
    "def fence_wo(val):\n",
    "    if val == \"GdWo\":\n",
    "        return 2\n",
    "    elif val == \"MnWw\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def fence_prv(val):\n",
    "    if val == \"GdPrv\":\n",
    "        return 2\n",
    "    elif val == \"MnPrv\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "train[\"Fence_wo\"] = train[\"Fence\"].apply(fence_wo)\n",
    "test[\"Fence_wo\"] = test[\"Fence\"].apply(fence_wo)\n",
    "train[\"Fence_Prv\"] = train[\"Fence\"].apply(fence_prv)\n",
    "test[\"Fence_Prv\"] = test[\"Fence\"].apply(fence_prv)\n",
    "\n",
    "mapping = {\"GdWo\": 2, \"MnPrv\": 1, \"GdPrv\": 2, \"MnWw\": 1, \"None\": 0}\n",
    "train[\"Fence\"] = train[\"Fence\"].map(mapping)\n",
    "test[\"Fence\"] = test[\"Fence\"].map(mapping)\n",
    "\n",
    "train[\"MoSold_sin\"] = np.sin(2 * np.pi * train[\"MoSold\"] / 12)\n",
    "train[\"MoSold_cos\"] = np.cos(2 * np.pi * train[\"MoSold\"] / 12)\n",
    "test[\"MoSold_sin\"] = np.sin(2 * np.pi * test[\"MoSold\"] / 12)\n",
    "test[\"MoSold_cos\"] = np.cos(2 * np.pi * test[\"MoSold\"] / 12)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Feature engineering \n",
    "train[\"TotalArea\"] = train[\"GrLivArea\"] + train[\"TotalBsmtSF\"] + train[\"GarageArea\"]\n",
    "train[\"BathPerRoom\"] = (train[\"FullBath\"] + train[\"HalfBath\"]) / (train[\"TotRmsAbvGrd\"] + 1)\n",
    "\n",
    "test[\"TotalArea\"] = test[\"GrLivArea\"] + test[\"TotalBsmtSF\"] + test[\"GarageArea\"]\n",
    "test[\"BathPerRoom\"] = (test[\"FullBath\"] + test[\"HalfBath\"]) / (test[\"TotRmsAbvGrd\"] + 1)\n",
    "\n",
    "train[\"OverallQual_GrLiv\"] = train[\"OverallQual\"] * train[\"GrLivArea\"]\n",
    "train[\"Qual_x_Bath\"] = train[\"OverallQual\"] * (train[\"FullBath\"] + train[\"HalfBath\"])\n",
    "\n",
    "test[\"OverallQual_GrLiv\"] = test[\"OverallQual\"] * test[\"GrLivArea\"]\n",
    "test[\"Qual_x_Bath\"] = test[\"OverallQual\"] * (test[\"FullBath\"] + test[\"HalfBath\"])\n",
    "\n",
    "neighborhood_means = pd.concat([train, y], axis=1).groupby(\"Neighborhood\")[\"SalePrice\"].mean()\n",
    "train[\"Neighborhood_mean\"] = train[\"Neighborhood\"].map(neighborhood_means)\n",
    "test[\"Neighborhood_mean\"] = test[\"Neighborhood\"].map(neighborhood_means)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sale type - replace NaN with mode (WD)\n",
    "train[\"SaleType\"] = train[\"SaleType\"].replace(np.nan, \"WD\")\n",
    "test[\"SaleType\"] = test[\"SaleType\"].replace(np.nan, \"WD\")\n",
    "\n",
    "train = train.drop(columns=[\"BsmtFinType2\",\"ExterCond\",\"PoolArea\",\"OpenPorchSF\",\"WoodDeckSF\",\"LowQualFinSF\",\"YearRemodAdd\",\"YearBuilt\",\"GarageYrBlt\",\"PoolQC\",\"Fence\",\"Functional\",\"GarageArea\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\",\"MiscFeature\",\"MiscVal\",\"RoofMatl\",\"Condition2\",\"Alley\", \"Street\", \"Utilities\", \"MiscFeature\"])\n",
    "test = test.drop(columns=[\"BsmtFinType2\",\"ExterCond\",\"PoolArea\",\"OpenPorchSF\",\"WoodDeckSF\",\"LowQualFinSF\",\"YearRemodAdd\",\"YearBuilt\",\"GarageYrBlt\",\"PoolQC\",\"Fence\",\"Functional\",\"GarageArea\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\",\"MiscFeature\",\"MiscVal\",\"RoofMatl\",\"Condition2\",\"Alley\", \"Street\", \"Utilities\", \"MiscFeature\"])\n",
    "\n",
    "print(test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62d7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#je garde à contre coeur : LandSlope, Screeporch PavedDrive?\n",
    "# truc potentiellement intéréssant : MiscVal kitchenAbove\n",
    "# truc ou je me suis permis des dinguerie : PoolQC (et donc poolarea) \n",
    "num_cols = [\"Neighborhood_mean\",\"Qual_x_Bath\",\"OverallQual_GrLiv\",\"BathPerRoom\",\"TotalArea\",\"Fence_wo\",\"Fence_Prv\",\"GarageQual\",\"KitchenQual\",\"BsmtFinType1\",\"BsmtFinSF2\",\"MasVnrArea\",\"ExterQual\",\"YrSold\",\"BsmtQual\",\"BsmtCond\",\"FireplaceQu\",\"GarageFinish\",\"BsmtExposure\",\"BsmtFinSF1\",\"BsmtUnfSF\",\"TotalBsmtSF\",\"GarageCond\",\"HeatingQC_num\",\"1stFlrSF\",\"2ndFlrSF\",\"GrLivArea\",\"BsmtFullBath\",\"BsmtHalfBath\",\"FullBath\",\"HalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"TotRmsAbvGrd\",\"Fireplaces\",\"GarageAge\",\"GarageCars\",\"WoodDeckSF_log\",\"MoSold_sin\",\"MoSold_cos\",\"Age_Built\",\"Age_RemodAdd\",\"LotFrontage\",\"LotArea\",\"OverallQual\",\"OverallCond\"]\n",
    "cat_cols = [\"MasVnrType\",\"Foundation\",\"HasBsmtFinSF1\",\"Heating\",\"CentralAir\",\"Electrical\",\"HasBsmtFinSF2\",\"HasLowQualFinSF\",\"GarageType\",\"PavedDrive\",\"HasScreenPorch\",\"GotPool\",\"SaleType\",\"SaleCondition\",\"Exterior1st\",\"Exterior2nd\",\"RoofStyle\",\"MSSubClass\",\"HouseStyle\",\"BldgType\",\"MSSubClass\",\"LotShape\",\"LandContour\",\"LotConfig\",\"LandSlope\",\"Condition1\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # One-hot categorical features\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        # Pass-through numerical features\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"  # drop unused raw columns like Name, Ticket, Cabin, etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e93c6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train columns: ['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'RoofStyle', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'ExterQual', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageCars', 'GarageQual', 'GarageCond', 'PavedDrive', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'Age_Built', 'Age_RemodAdd', 'HasBsmtFinSF2', 'HasBsmtFinSF1', 'HeatingQC_num', 'HasLowQualFinSF', 'GarageAge', 'WoodDeckSF_log', 'OpenPorchSF_log', 'HasScreenPorch', 'ScreenPorchLog', 'GotPool', 'Fence_wo', 'Fence_Prv', 'MoSold_sin', 'MoSold_cos', 'TotalArea', 'BathPerRoom', 'OverallQual_GrLiv', 'Qual_x_Bath', 'Neighborhood_mean']\n",
      "X_test columns: ['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'RoofStyle', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'ExterQual', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageCars', 'GarageQual', 'GarageCond', 'PavedDrive', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'Age_Built', 'Age_RemodAdd', 'HasBsmtFinSF2', 'HasBsmtFinSF1', 'HeatingQC_num', 'HasLowQualFinSF', 'GarageAge', 'WoodDeckSF_log', 'OpenPorchSF_log', 'HasScreenPorch', 'ScreenPorchLog', 'GotPool', 'Fence_wo', 'Fence_Prv', 'MoSold_sin', 'MoSold_cos', 'TotalArea', 'BathPerRoom', 'OverallQual_GrLiv', 'Qual_x_Bath', 'Neighborhood_mean']\n",
      "X_train shape: (1460, 77)\n",
      "y_train shape: (1460,)\n",
      "X_test shape: (1459, 77)\n",
      "Colonnes manquantes dans test: set()\n",
      "Colonnes manquantes dans train: set()\n",
      "RMSE par fold: [28901.84422441 28966.80179248 45021.10928392 26484.54188408\n",
      " 23786.99351528]\n",
      "RMSE moyen: 30632.25814003384\n"
     ]
    }
   ],
   "source": [
    "X_train = train\n",
    "X_train2= X_train.copy()\n",
    "y_train = y\n",
    "y_train2 = y_train.copy()\n",
    "X_test = test\n",
    "X_test2 = X_test.copy()\n",
    "\n",
    "\n",
    "print(\"X_train columns:\", X_train.columns.tolist())\n",
    "print(\"X_test columns:\", X_test.columns.tolist())\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "missing_in_test = set(X_train.columns) - set(X_test.columns)\n",
    "missing_in_train = set(X_test.columns) - set(X_train.columns)\n",
    "print(\"Colonnes manquantes dans test:\", missing_in_test)\n",
    "print(\"Colonnes manquantes dans train:\", missing_in_train)\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=500, max_depth=None, random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42),\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"Lasso\": Lasso(alpha=0.0005, max_iter=5000),\n",
    "    \"SVR\": SVR(kernel=\"rbf\", C=20, epsilon=0.1),\n",
    "    \"KNN\": KNeighborsRegressor(n_neighbors=5),\n",
    "    \"XGB\": xgb.XGBRegressor(n_estimators=2000, learning_rate=0.05, max_depth=4, subsample=0.7, colsample_bytree=0.7, random_state=42),\n",
    "    \"LGBM\": lgb.LGBMRegressor(n_estimators=2000, learning_rate=0.05, max_depth=-1, subsample=0.7, colsample_bytree=0.7, random_state=42),\n",
    "}\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", model)\n",
    "])\n",
    "\n",
    "# 5-fold CV\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scores = cross_val_score(\n",
    "    pipeline, X_train, y_train,\n",
    "    cv=cv, scoring=\"neg_root_mean_squared_error\"\n",
    ")\n",
    "\n",
    "print(\"RMSE par fold:\", -scores)\n",
    "print(\"RMSE moyen:\", -np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37b0081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "257a8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"SalePrice\": y_pred\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d380cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:   # Change to True to run all models and save their predictions\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        pipeline = Pipeline(steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "        \n",
    "        # --- Cross-validation ---\n",
    "        scores = cross_val_score(\n",
    "            pipeline, X_train2, y_train2,\n",
    "            cv=cv, scoring=\"neg_root_mean_squared_error\", n_jobs=4\n",
    "        )\n",
    "        \n",
    "        rmse_scores = -scores\n",
    "        mean_rmse, std_rmse = rmse_scores.mean(), rmse_scores.std()\n",
    "        print(f\"{name}: RMSE = {mean_rmse:.4f} ± {std_rmse:.4f}\")\n",
    "        \n",
    "        # --- Train full model and predict on X_test ---\n",
    "        pipeline.fit(X_train2, y_train2)\n",
    "        y_pred = pipeline.predict(X_test2)\n",
    "        \n",
    "        submission = pd.DataFrame({\n",
    "            \"Id\": test_ids,        # Assumes you have test_ids defined\n",
    "            \"SalePrice\": y_pred\n",
    "        })\n",
    "        \n",
    "        filename = f\"submission_{name}.csv\"\n",
    "        submission.to_csv(filename, index=False)\n",
    "        print(f\"Prédictions sauvegardées dans {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9731b7bc",
   "metadata": {},
   "source": [
    "# Autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8901f408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HasScreenPorch     ScreenPorchLog       0.996544\n",
      "OverallQual        OverallQual_GrLiv    0.989678\n",
      "BsmtFinSF2         HasBsmtFinSF2        0.988754\n",
      "BsmtFinSF1         HasBsmtFinSF1        0.969787\n",
      "GarageQual         GarageCond           0.959172\n",
      "GarageCars         TotalArea            0.882630\n",
      "Fireplaces         FireplaceQu          0.863241\n",
      "BsmtFinType1       HasBsmtFinSF1        0.854392\n",
      "                   BsmtFinSF1           0.852712\n",
      "OverallQual_GrLiv  Qual_x_Bath          0.838232\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculer la matrice de corrélation pour les variables numériques\n",
    "corr_matrix = X_train.corr(numeric_only=True).abs()\n",
    "\n",
    "# On ne garde que la partie supérieure de la matrice (pour éviter les doublons)\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Trouver les paires avec la plus forte corrélation (hors diagonale)\n",
    "most_correlated = (\n",
    "    upper.stack()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "print(most_correlated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da291ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250928_141948\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.5\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Memory Avail:       11.14 GB / 31.92 GB (34.9%)\n",
      "Disk Space Avail:   795.91 GB / 1861.39 GB (42.8%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 375s of the 1500s of remaining time (25%).\n",
      "2025-09-28 16:19:49,133\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-09-28 16:19:51,908\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"c:\\Users\\Victor\\Desktop\\Kaggle\\house-prices-advanced-regression-techniques\\AutogluonModels\\ag-20250928_141948\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Beginning AutoGluon training ... Time limit = 370s\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m AutoGluon will save models to \"c:\\Users\\Victor\\Desktop\\Kaggle\\house-prices-advanced-regression-techniques\\AutogluonModels\\ag-20250928_141948\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Train Data Rows:    1297\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Train Data Columns: 77\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Label Column:       SalePrice\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tAvailable Memory:                    10093.49 MB\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tTrain Data (Original)  Memory Usage: 1.99 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t\t\tNote: Converting 6 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t\t('float', [])  : 20 | ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', ...]\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t\t('int', [])    : 35 | ['MSSubClass', 'OverallQual', 'OverallCond', 'ExterQual', 'BsmtQual', ...]\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t\t('object', []) : 22 | ['MSZoning', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', ...]\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t\t('category', [])  : 21 | ['MSZoning', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', ...]\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t\t('float', [])     : 20 | ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', ...]\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t\t('int', [])       : 30 | ['MSSubClass', 'OverallQual', 'OverallCond', 'ExterQual', 'BsmtQual', ...]\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t\t('int', ['bool']) :  6 | ['CentralAir', 'HasBsmtFinSF2', 'HasBsmtFinSF1', 'HasLowQualFinSF', 'HasScreenPorch', ...]\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.1s = Fit runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t77 features in original data used to generate 77 features in processed data.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tTrain Data (Processed) Memory Usage: 0.54 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Data preprocessing and feature engineering runtime = 0.15s ...\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 246.55s of the 369.91s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.20%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=9532)\u001b[0m [1000]\tvalid_set's rmse: 28433.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-24898.3526\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t6.04s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.21s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 228.73s of the 352.09s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.20%)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-27021.6016\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t1.4s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 224.47s of the 347.83s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-28614.1102\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.99s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.14s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 223.27s of the 346.62s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=4.28%)\n",
      "\u001b[36m(_ray_fit pid=26636)\u001b[0m \tRan out of time, early stopping on iteration 322.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-24961.5081\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t23.11s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 197.60s of the 320.96s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-27700.0511\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.58s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.21s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 196.73s of the 320.08s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.06%)\n",
      "\u001b[36m(_ray_fit pid=19072)\u001b[0m No improvement since epoch 6: early stopping\n",
      "\u001b[36m(_ray_fit pid=31528)\u001b[0m \tRan out of time, early stopping on iteration 330.\u001b[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-26359.0922\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t12.26s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.11s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 181.59s of the 304.95s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.42%)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-27161.7313\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t8.19s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 170.28s of the 293.64s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\u001b[36m(_ray_fit pid=15568)\u001b[0m c:\\Users\\Victor\\Desktop\\Kaggle\\.venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=15568)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-25731.6923\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t21.46s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.21s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 146.25s of the 269.61s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=15324)\u001b[0m c:\\Users\\Victor\\Desktop\\Kaggle\\.venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=15324)\u001b[0m   warnings.warn(\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=25944)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 112)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.70%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=31200)\u001b[0m [1000]\tvalid_set's rmse: 38060.9\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-30441.3345\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t6.21s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 136.92s of the 260.28s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=4.27%)\n",
      "\u001b[36m(_ray_fit pid=17072)\u001b[0m \tRan out of time, early stopping on iteration 320.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-24332.5509\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t14.62s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 119.80s of the 243.16s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\u001b[36m(_ray_fit pid=21060)\u001b[0m c:\\Users\\Victor\\Desktop\\Kaggle\\.venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=21060)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=8440)\u001b[0m \tRan out of time, early stopping on iteration 318.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=21868)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 45)\n",
      "\u001b[36m(_ray_fit pid=16252)\u001b[0m c:\\Users\\Victor\\Desktop\\Kaggle\\.venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16252)\u001b[0m   warnings.warn(\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-27145.5046\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t16.34s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.2s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 100.63s of the 223.98s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.37%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=23624)\u001b[0m [1000]\tvalid_set's rmse: 21993.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-26747.4786\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t3.93s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 93.04s of the 216.40s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.06%)\n",
      "\u001b[36m(_ray_fit pid=16252)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 47)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16072)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 35)\n",
      "\u001b[36m(_ray_fit pid=9776)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 33)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-27266.1363\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t14.64s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 75.65s of the 199.01s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=6.33%)\n",
      "\u001b[36m(_ray_fit pid=27656)\u001b[0m \tRan out of time, early stopping on iteration 39.\n",
      "\u001b[36m(_ray_fit pid=26496)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 31)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-43365.7858\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t8.24s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 64.34s of the 187.70s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.13%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=20324)\u001b[0m [1000]\tvalid_set's rmse: 21368.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-25276.1545\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t7.04s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.31s\t = Validation runtime\n",
      "\u001b[36m(_ray_fit pid=4240)\u001b[0m \tRan out of time, early stopping on iteration 39.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 53.65s of the 177.01s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tTime limit exceeded... Skipping NeuralNetTorch_r22_BAG_L1.\n",
      "\u001b[36m(_ray_fit pid=18036)\u001b[0m \tWarning: Model has no time left to train, skipping model... (Time Left = -0.0s)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 44.58s of the 167.94s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=2.41%)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-28327.817\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t5.34s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.14s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 36.61s of the 159.97s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-27532.287\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.6s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.17s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 35.76s of the 159.12s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=4.48%)\n",
      "\u001b[36m(_ray_fit pid=8676)\u001b[0m \tRan out of time, early stopping on iteration 93.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-28501.0084\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t4.4s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 28.78s of the 152.14s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.07%)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tTime limit exceeded... Skipping NeuralNetFastAI_r102_BAG_L1.\n",
      "\u001b[36m(_ray_fit pid=16092)\u001b[0m \tRan out of time, early stopping on iteration 95.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=28828)\u001b[0m \tWarning: Model has no time left to train, skipping model... (Time Left = -2.1s)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 20.68s of the 144.03s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=5.43%)\n",
      "\u001b[36m(_ray_fit pid=10468)\u001b[0m \tRan out of time, early stopping on iteration 15.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-66393.7361\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t2.84s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 14.90s of the 138.25s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-28903.1867\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.89s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.18s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 13.71s of the 137.07s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.75%)\n",
      "\u001b[36m(_ray_fit pid=3516)\u001b[0m \tRan out of time, early stopping on iteration 851. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=3516)\u001b[0m \t[610]\tvalid_set's rmse: 21536.9\n",
      "\u001b[36m(_ray_fit pid=30332)\u001b[0m \tRan out of time, early stopping on iteration 17.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-25729.3648\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t2.99s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.13s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 7.38s of the 130.74s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.06%)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tTime limit exceeded... Skipping NeuralNetFastAI_r145_BAG_L1.\n",
      "\u001b[36m(_ray_fit pid=21836)\u001b[0m \tRan out of time, early stopping on iteration 819. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=21836)\u001b[0m \t[819]\tvalid_set's rmse: 31738.9\n",
      "\u001b[36m(_ray_fit pid=3300)\u001b[0m \tWarning: Model has no time left to train, skipping model... (Time Left = -4.4s)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 122.42s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.273, 'CatBoost_r177_BAG_L1': 0.273, 'NeuralNetTorch_BAG_L1': 0.182, 'NeuralNetFastAI_BAG_L1': 0.136, 'NeuralNetTorch_r79_BAG_L1': 0.136}\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-23275.1742\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.02s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 122.38s of the 122.30s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.25%)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-26247.5643\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t1.62s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.06s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 118.03s of the 117.96s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.27%)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-25811.526\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t2.15s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.05s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 113.08s of the 113.00s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-24002.7426\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t1.08s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.15s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 111.78s of the 111.71s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=5.09%)\n",
      "\u001b[36m(_ray_fit pid=7700)\u001b[0m \tRan out of time, early stopping on iteration 162.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-25057.901\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t12.07s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 97.08s of the 97.01s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-23764.0069\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.6s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.16s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 96.26s of the 96.19s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.08%)\n",
      "\u001b[36m(_ray_fit pid=10740)\u001b[0m No improvement since epoch 3: early stopping\n",
      "\u001b[36m(_ray_fit pid=3856)\u001b[0m \tRan out of time, early stopping on iteration 164.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-29045.0003\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t12.9s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.13s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 80.69s of the 80.62s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.57%)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-25274.325\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t4.69s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 72.46s of the 72.38s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=19388)\u001b[0m No improvement since epoch 9: early stopping\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.04%)\n",
      "\u001b[36m(_ray_fit pid=28828)\u001b[0m c:\\Users\\Victor\\Desktop\\Kaggle\\.venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "\u001b[36m(_ray_fit pid=28828)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tTime limit exceeded... Skipping NeuralNetTorch_BAG_L2.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 60.34s of the 60.27s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=1.12%)\n",
      "\u001b[36m(_ray_fit pid=3684)\u001b[0m \tRan out of time, early stopping on iteration 949. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=3684)\u001b[0m \t[947]\tvalid_set's rmse: 28223.5\n",
      "\u001b[36m(_ray_fit pid=25708)\u001b[0m c:\\Users\\Victor\\Desktop\\Kaggle\\.venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=25708)\u001b[0m   warnings.warn(\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-26741.088\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t7.67s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 49.52s of the 49.44s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=5.72%)\n",
      "\u001b[36m(_ray_fit pid=11236)\u001b[0m \tRan out of time, early stopping on iteration 89.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-24908.7949\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t5.83s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 40.91s of the 40.84s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.06%)\n",
      "\u001b[36m(_ray_fit pid=28248)\u001b[0m \tWarning: Model has no time left to train, skipping model... (Time Left = -1.1s)\n",
      "\u001b[36m(_ray_fit pid=23380)\u001b[0m \tRan out of time, early stopping on iteration 81.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tTime limit exceeded... Skipping NeuralNetTorch_r79_BAG_L2.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 32.78s of the 32.71s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.49%)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-25538.7362\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t2.96s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.07s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 26.98s of the 26.91s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.09%)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tTime limit exceeded... Skipping NeuralNetFastAI_r191_BAG_L2.\n",
      "\u001b[36m(_ray_fit pid=6544)\u001b[0m \tWarning: Model has no time left to train, skipping model... (Time Left = -2.5s)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 19.15s of the 19.07s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=7.07%)\n",
      "\u001b[36m(_ray_fit pid=16600)\u001b[0m \tRan out of time, early stopping on iteration 4.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-70295.0077\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t2.67s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.1s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: LightGBM_r96_BAG_L2 ... Training model for up to 13.97s of the 13.89s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.20%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=30744)\u001b[0m [1000]\tvalid_set's rmse: 28703.3\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=17788)\u001b[0m \tRan out of time, early stopping on iteration 2343. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=17788)\u001b[0m \t[2093]\tvalid_set's rmse: 19001.1\n",
      "\u001b[36m(_ray_fit pid=26292)\u001b[0m \tRan out of time, early stopping on iteration 5.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-26999.4661\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t2.66s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.12s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: NeuralNetTorch_r22_BAG_L2 ... Training model for up to 7.99s of the 7.92s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.05%)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tTime limit exceeded... Skipping NeuralNetTorch_r22_BAG_L2.\n",
      "\u001b[36m(_ray_fit pid=28276)\u001b[0m \tWarning: Model has no time left to train, skipping model... (Time Left = -4.3s)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -0.13s of remaining time.\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \tEnsemble Weights: {'RandomForestMSE_BAG_L2': 0.273, 'LightGBMXT_BAG_L1': 0.227, 'CatBoost_r177_BAG_L1': 0.182, 'NeuralNetTorch_BAG_L1': 0.136, 'NeuralNetTorch_r79_BAG_L1': 0.091, 'NeuralNetFastAI_BAG_L1': 0.045, 'ExtraTreesMSE_BAG_L2': 0.045}\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t-23138.676\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.02s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m AutoGluon training complete, total runtime = 370.22s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 95.7 rows/s (163 batch size)\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\Victor\\Desktop\\Kaggle\\house-prices-advanced-regression-techniques\\AutogluonModels\\ag-20250928_141948\\ds_sub_fit\\sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=16636)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                          model  score_holdout     score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0        RandomForestMSE_BAG_L2  -22894.617587 -24002.742593  root_mean_squared_error        5.027377       2.429729  133.176190                 0.110616                0.152779           1.084890            2       True         25\n",
      "1          ExtraTreesMSE_BAG_L2  -24745.211262 -23764.006949  root_mean_squared_error        5.057084       2.432568  132.687799                 0.140323                0.155618           0.596498            2       True         27\n",
      "2                XGBoost_BAG_L2  -26764.444427 -25274.325001  root_mean_squared_error        5.200973       2.349774  136.783598                 0.284212                0.072824           4.692298            2       True         29\n",
      "3          LightGBMLarge_BAG_L2  -26842.448820 -26741.088042  root_mean_squared_error        5.212687       2.364985  139.762927                 0.295926                0.088035           7.671627            2       True         30\n",
      "4          CatBoost_r177_BAG_L1  -26991.886236 -24332.550918  root_mean_squared_error        0.093566       0.098035   14.620716                 0.093566                0.098035          14.620716            1       True         10\n",
      "5                XGBoost_BAG_L1  -27015.663693 -27161.731315  root_mean_squared_error        0.520285       0.103560    8.185148                 0.520285                0.103560           8.185148            1       True          7\n",
      "6               CatBoost_BAG_L2  -27042.024403 -25057.901045  root_mean_squared_error        5.021777       2.358012  144.164202                 0.105016                0.081063          12.072901            2       True         26\n",
      "7           WeightedEnsemble_L3  -27251.978771 -23138.675959  root_mean_squared_error        5.177513       2.585347  133.792310                 0.009813                0.000000           0.019621            3       True         35\n",
      "8          CatBoost_r177_BAG_L2  -27574.498276 -24908.794902  root_mean_squared_error        5.006592       2.368820  137.922425                 0.089831                0.091871           5.831125            2       True         31\n",
      "9               CatBoost_BAG_L1  -28049.158545 -24961.508136  root_mean_squared_error        0.518720       0.080909   23.112677                 0.518720                0.080909          23.112677            1       True          4\n",
      "10         LightGBM_r188_BAG_L1  -29114.100945 -25729.364774  root_mean_squared_error        0.391277       0.125968    2.992163                 0.391277                0.125968           2.992163            1       True         21\n",
      "11       NeuralNetFastAI_BAG_L2  -29172.016119 -29045.000276  root_mean_squared_error        5.265975       2.404934  144.992476                 0.349214                0.127985          12.901176            2       True         28\n",
      "12     RandomForest_r195_BAG_L1  -29313.055197 -28903.186743  root_mean_squared_error        0.101793       0.183527    0.891061                 0.101793                0.183527           0.891061            1       True         20\n",
      "13       NeuralNetFastAI_BAG_L1  -29534.297311 -26359.092182  root_mean_squared_error        0.952735       0.107361   12.257943                 0.952735                0.107361          12.257943            1       True          6\n",
      "14       RandomForestMSE_BAG_L1  -29607.813207 -28614.110175  root_mean_squared_error        0.111254       0.139379    0.989180                 0.111254                0.139379           0.989180            1       True          3\n",
      "15         LightGBMLarge_BAG_L1  -29661.245740 -30441.334515  root_mean_squared_error        0.312490       0.098821    6.212217                 0.312490                0.098821           6.212217            1       True          9\n",
      "16          WeightedEnsemble_L2  -29715.432201 -23275.174235  root_mean_squared_error        2.216490       0.823600   70.740339                 0.008547                0.000000           0.019876            2       True         22\n",
      "17            LightGBMXT_BAG_L1  -29773.032836 -24898.352577  root_mean_squared_error        0.724107       0.208636    6.038105                 0.724107                0.208636           6.038105            1       True          1\n",
      "18        ExtraTrees_r42_BAG_L1  -30187.603102 -27532.286971  root_mean_squared_error        0.106412       0.172799    0.596279                 0.106412                0.172799           0.596279            1       True         17\n",
      "19  NeuralNetFastAI_r191_BAG_L1  -30252.482457 -27266.136301  root_mean_squared_error        0.275984       0.119777   14.643340                 0.275984                0.119777          14.643340            1       True         13\n",
      "20         ExtraTreesMSE_BAG_L1  -30370.747328 -27700.051055  root_mean_squared_error        0.115994       0.205076    0.580766                 0.115994                0.205076           0.580766            1       True          5\n",
      "21         LightGBM_r131_BAG_L2  -30527.250639 -25538.736174  root_mean_squared_error        5.158551       2.345591  135.055751                 0.241790                0.068642           2.964451            2       True         32\n",
      "22           XGBoost_r33_BAG_L1  -31018.913866 -28327.817046  root_mean_squared_error        0.343474       0.141828    5.336152                 0.343474                0.141828           5.336152            1       True         16\n",
      "23              LightGBM_BAG_L1  -31195.918906 -27021.601565  root_mean_squared_error        0.123137       0.063546    1.396974                 0.123137                0.063546           1.396974            1       True          2\n",
      "24         LightGBM_r131_BAG_L1  -31272.329294 -26747.478601  root_mean_squared_error        0.330288       0.119215    3.926135                 0.330288                0.119215           3.926135            1       True         12\n",
      "25          LightGBM_r96_BAG_L1  -32267.639829 -25276.154467  root_mean_squared_error        0.614900       0.306494    7.036234                 0.614900                0.306494           7.036234            1       True         15\n",
      "26         CatBoost_r137_BAG_L1  -33079.979887 -28501.008416  root_mean_squared_error        0.094973       0.080755    4.397854                 0.094973                0.080755           4.397854            1       True         18\n",
      "27              LightGBM_BAG_L2  -33098.485057 -25811.526035  root_mean_squared_error        5.041694       2.330188  134.237090                 0.124933                0.053239           2.145790            2       True         24\n",
      "28            LightGBMXT_BAG_L2  -33558.305406 -26247.564326  root_mean_squared_error        5.036994       2.337780  133.708338                 0.120233                0.060831           1.617038            2       True         23\n",
      "29        NeuralNetTorch_BAG_L1  -34088.312991 -25731.692290  root_mean_squared_error        0.211632       0.206391   21.460012                 0.211632                0.206391          21.460012            1       True          8\n",
      "30    NeuralNetTorch_r79_BAG_L1  -35483.634299 -27145.504592  root_mean_squared_error        0.225904       0.203177   16.343688                 0.225904                0.203177          16.343688            1       True         11\n",
      "31          LightGBM_r96_BAG_L2  -37203.228737 -26999.466072  root_mean_squared_error        5.172147       2.397670  134.755790                 0.255386                0.120720           2.664490            2       True         34\n",
      "32           CatBoost_r9_BAG_L1  -54418.477068 -43365.785830  root_mean_squared_error        0.095104       0.069117    8.240690                 0.095104                0.069117           8.240690            1       True         14\n",
      "33          CatBoost_r13_BAG_L1  -80199.461916 -66393.736109  root_mean_squared_error        0.083310       0.098078    2.835528                 0.083310                0.098078           2.835528            1       True         19\n",
      "34           CatBoost_r9_BAG_L2  -84478.239386 -70295.007695  root_mean_squared_error        4.999615       2.377513  134.764776                 0.082854                0.100564           2.673475            2       True         33\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t387s\t = DyStack   runtime |\t1113s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 1113s\n",
      "AutoGluon will save models to \"c:\\Users\\Victor\\Desktop\\Kaggle\\house-prices-advanced-regression-techniques\\AutogluonModels\\ag-20250928_141948\"\n",
      "Train Data Rows:    1460\n",
      "Train Data Columns: 77\n",
      "Label Column:       SalePrice\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    8125.09 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.25 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 6 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 20 | ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', ...]\n",
      "\t\t('int', [])    : 35 | ['MSSubClass', 'OverallQual', 'OverallCond', 'ExterQual', 'BsmtQual', ...]\n",
      "\t\t('object', []) : 22 | ['MSZoning', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 21 | ['MSZoning', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', ...]\n",
      "\t\t('float', [])     : 20 | ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', ...]\n",
      "\t\t('int', [])       : 30 | ['MSSubClass', 'OverallQual', 'OverallCond', 'ExterQual', 'BsmtQual', ...]\n",
      "\t\t('int', ['bool']) :  6 | ['CentralAir', 'HasBsmtFinSF2', 'HasBsmtFinSF1', 'HasLowQualFinSF', 'HasScreenPorch', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t77 features in original data used to generate 77 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.61 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.33s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 741.87s of the 1113.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.26%)\n",
      "\t-24726.7801\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.53s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 724.24s of the 1095.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.26%)\n",
      "\t-26574.5616\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.13s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 709.62s of the 1080.82s of remaining time.\n",
      "\t-28704.7945\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.8s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 708.62s of the 1079.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=5.59%)\n",
      "\t-24101.1821\t = Validation score   (-root_mean_squared_error)\n",
      "\t71.7s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 634.38s of the 1005.59s of remaining time.\n",
      "\t-27854.5319\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 633.60s of the 1004.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.09%)\n",
      "\t-30574.1339\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.73s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 617.86s of the 989.06s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.63%)\n",
      "\t-26701.2151\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.79s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 610.10s of the 981.30s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.05%)\n",
      "\t-26734.6205\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.41s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 584.18s of the 955.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.89%)\n",
      "\t-26531.0393\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.21s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 558.31s of the 929.51s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=5.37%)\n",
      "\t-24759.2049\t = Validation score   (-root_mean_squared_error)\n",
      "\t56.68s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 499.17s of the 870.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.04%)\n",
      "\t-28344.6553\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.34s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 477.19s of the 848.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.42%)\n",
      "\t-26715.4846\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.29s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 456.07s of the 827.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.08%)\n",
      "\t-30540.4249\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.66s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 437.91s of the 809.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=7.39%)\n",
      "\t-25515.8341\t = Validation score   (-root_mean_squared_error)\n",
      "\t44.64s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 390.41s of the 761.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.16%)\n",
      "\t-25216.2265\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.95s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 379.75s of the 750.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.04%)\n",
      "\t-26381.1852\t = Validation score   (-root_mean_squared_error)\n",
      "\t32.14s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 345.13s of the 716.34s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=2.82%)\n",
      "\t-27269.753\t = Validation score   (-root_mean_squared_error)\n",
      "\t35.67s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 306.65s of the 677.85s of remaining time.\n",
      "\t-28004.0132\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.5s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 305.97s of the 677.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=4.63%)\n",
      "\t-24824.8921\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.57s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 271.59s of the 642.80s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.08%)\n",
      "\t-30111.1943\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.28s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 246.47s of the 617.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=6.68%)\n",
      "\t-27372.1237\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.31s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 218.13s of the 589.33s of remaining time.\n",
      "\t-28823.5937\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.79s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 217.03s of the 588.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.85%)\n",
      "\t-24657.6629\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.66s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 205.18s of the 576.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.08%)\n",
      "\t-30875.9301\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.17s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 187.23s of the 558.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.53%)\n",
      "\t-25588.9494\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.05s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 178.94s of the 550.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.04%)\n",
      "\t-28503.7758\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.17s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 154.13s of the 525.34s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.60%)\n",
      "\t-25588.4214\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.86s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 141.63s of the 512.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.04%)\n",
      "\t-25304.0917\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.49s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 120.58s of the 491.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=5.19%)\n",
      "\t-25561.0036\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.94s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 104.86s of the 476.06s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.09%)\n",
      "\t-28513.62\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.95s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 86.10s of the 457.30s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=1.02%)\n",
      "\t-26693.9038\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.62s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 76.20s of the 447.41s of remaining time.\n",
      "\t-28773.2067\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 75.59s of the 446.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=4.78%)\n",
      "\t-25289.476\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.37s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 64.69s of the 435.90s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.09%)\n",
      "\t-39358.8836\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.05s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 49.56s of the 420.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.06%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_r14_BAG_L1.\n",
      "Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 41.08s of the 412.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=1.63%)\n",
      "\t-26308.8809\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.14s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 31.53s of the 402.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.08%)\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_r143_BAG_L1.\n",
      "Fitting model: CatBoost_r70_BAG_L1 ... Training model for up to 23.79s of the 395.00s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=5.27%)\n",
      "\t-49585.1795\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.1s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r156_BAG_L1 ... Training model for up to 18.08s of the 389.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.09%)\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_r156_BAG_L1.\n",
      "Fitting model: LightGBM_r196_BAG_L1 ... Training model for up to 10.01s of the 381.22s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.88%)\n",
      "\t-26987.3119\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.5s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: RandomForest_r39_BAG_L1 ... Training model for up to 4.45s of the 375.66s of remaining time.\n",
      "\t-28892.6027\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.69s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: CatBoost_r167_BAG_L1 ... Training model for up to 3.53s of the 374.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=5.65%)\n",
      "\tTime limit exceeded... Skipping CatBoost_r167_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 370.80s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.24, 'NeuralNetTorch_r86_BAG_L1': 0.24, 'LightGBMXT_BAG_L1': 0.16, 'NeuralNetTorch_r22_BAG_L1': 0.12, 'XGBoost_r194_BAG_L1': 0.12, 'LightGBM_r188_BAG_L1': 0.08, 'NeuralNetFastAI_r11_BAG_L1': 0.04}\n",
      "\t-23037.9712\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 370.76s of the 370.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.33%)\n",
      "\t-25384.0426\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.88s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 355.54s of the 355.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.33%)\n",
      "\t-25794.3618\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.2s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 350.71s of the 350.58s of remaining time.\n",
      "\t-24351.6846\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.5s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 348.93s of the 348.80s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=5.42%)\n",
      "\t-24378.9235\t = Validation score   (-root_mean_squared_error)\n",
      "\t35.83s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 310.62s of the 310.49s of remaining time.\n",
      "\t-24072.0878\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 309.78s of the 309.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.10%)\n",
      "\t-27959.8419\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.57s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 293.64s of the 293.50s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.76%)\n",
      "\t-23835.5164\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.98s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 286.51s of the 286.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.07%)\n",
      "\t-26070.5505\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.29s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 262.32s of the 262.19s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=1.17%)\n",
      "\t-25186.7363\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.26s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 249.81s of the 249.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=5.52%)\n",
      "\t-24149.3566\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.79s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 221.44s of the 221.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.06%)\n",
      "\t-27654.5033\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.21s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 193.52s of the 193.38s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.53%)\n",
      "\t-24597.3082\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.63s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 182.82s of the 182.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.10%)\n",
      "\t-28651.921\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.25s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 164.04s of the 163.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=7.82%)\n",
      "\t-28389.2827\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.03s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L2 ... Training model for up to 143.97s of the 143.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t-25824.8292\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.93s\t = Training   runtime\n",
      "\t0.44s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L2 ... Training model for up to 131.09s of the 130.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.06%)\n",
      "\t-27129.9492\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.51s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L2 ... Training model for up to 110.98s of the 110.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=5.20%)\n",
      "\t-23938.0308\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.05s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L2 ... Training model for up to 95.88s of the 95.75s of remaining time.\n",
      "\t-24374.1939\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L2 ... Training model for up to 95.02s of the 94.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=5.47%)\n",
      "\t-24143.3523\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.39s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L2 ... Training model for up to 81.85s of the 81.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.12%)\n",
      "\t-29995.1192\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.56s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L2 ... Training model for up to 65.03s of the 64.90s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=9.26%)\n",
      "\t-48160.5891\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.93s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForest_r195_BAG_L2 ... Training model for up to 54.61s of the 54.48s of remaining time.\n",
      "\t-24095.0109\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.19s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L2 ... Training model for up to 53.15s of the 53.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=1.21%)\n",
      "\t-25940.276\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.79s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L2 ... Training model for up to 45.23s of the 45.10s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.10%)\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_r145_BAG_L2.\n",
      "Fitting model: XGBoost_r89_BAG_L2 ... Training model for up to 37.19s of the 37.05s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.62%)\n",
      "\t-23635.3601\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.19s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L2 ... Training model for up to 31.08s of the 30.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.07%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_r30_BAG_L2.\n",
      "Fitting model: LightGBM_r130_BAG_L2 ... Training model for up to 22.67s of the 22.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.86%)\n",
      "\t-26255.493\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.54s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L2 ... Training model for up to 16.28s of the 16.15s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.06%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_r86_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 7.80s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost_BAG_L2': 0.28, 'CatBoost_BAG_L1': 0.2, 'LightGBM_r188_BAG_L1': 0.16, 'NeuralNetTorch_r86_BAG_L1': 0.16, 'XGBoost_r89_BAG_L2': 0.12, 'NeuralNetFastAI_r11_BAG_L1': 0.04, 'NeuralNetTorch_BAG_L2': 0.04}\n",
      "\t-22839.1501\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1105.65s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 82.8 rows/s (183 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\Victor\\Desktop\\Kaggle\\house-prices-advanced-regression-techniques\\AutogluonModels\\ag-20250928_141948\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          model     score_val              eval_metric  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0           WeightedEnsemble_L3 -22839.150083  root_mean_squared_error       2.741976  275.192782                0.000000           0.014096            3       True         65\n",
      "1           WeightedEnsemble_L2 -23037.971240  root_mean_squared_error       1.348478  160.101720                0.001072           0.017421            2       True         39\n",
      "2            XGBoost_r89_BAG_L2 -23635.360087  root_mean_squared_error       2.441849  249.908161                0.089715           3.191523            2       True         63\n",
      "3                XGBoost_BAG_L2 -23835.516389  root_mean_squared_error       2.424677  250.696927                0.072543           3.980289            2       True         46\n",
      "4            XGBoost_r33_BAG_L2 -23938.030794  root_mean_squared_error       2.532021  258.767847                0.179887          12.051209            2       True         56\n",
      "5          ExtraTreesMSE_BAG_L2 -24072.087834  root_mean_squared_error       2.507117  247.332505                0.154983           0.615867            2       True         44\n",
      "6      RandomForest_r195_BAG_L2 -24095.010900  root_mean_squared_error       2.538687  247.904890                0.186553           1.188251            2       True         61\n",
      "7               CatBoost_BAG_L1 -24101.182143  root_mean_squared_error       0.088031   71.703602                0.088031          71.703602            1       True          4\n",
      "8          CatBoost_r137_BAG_L2 -24143.352306  root_mean_squared_error       2.447114  257.109409                0.094980          10.392771            2       True         58\n",
      "9          CatBoost_r177_BAG_L2 -24149.356619  root_mean_squared_error       2.444239  272.505746                0.092104          25.789108            2       True         49\n",
      "10       RandomForestMSE_BAG_L2 -24351.684561  root_mean_squared_error       2.533440  248.216573                0.181305           1.499935            2       True         42\n",
      "11        ExtraTrees_r42_BAG_L2 -24374.193900  root_mean_squared_error       2.537722  247.297256                0.185588           0.580618            2       True         57\n",
      "12              CatBoost_BAG_L2 -24378.923532  root_mean_squared_error       2.444754  282.547277                0.092620          35.830639            2       True         43\n",
      "13         LightGBM_r131_BAG_L2 -24597.308249  root_mean_squared_error       2.485347  254.349324                0.133213           7.632685            2       True         51\n",
      "14         LightGBM_r188_BAG_L1 -24657.662856  root_mean_squared_error       0.263200    7.660316                0.263200           7.660316            1       True         23\n",
      "15            LightGBMXT_BAG_L1 -24726.780089  root_mean_squared_error       0.330378    7.528907                0.330378           7.528907            1       True          1\n",
      "16         CatBoost_r177_BAG_L1 -24759.204942  root_mean_squared_error       0.073411   56.677448                0.073411          56.677448            1       True         10\n",
      "17         CatBoost_r137_BAG_L1 -24824.892137  root_mean_squared_error       0.087737   31.565050                0.087737          31.565050            1       True         19\n",
      "18         LightGBMLarge_BAG_L2 -25186.736319  root_mean_squared_error       2.453276  255.979130                0.101141           9.262491            2       True         48\n",
      "19          LightGBM_r96_BAG_L1 -25216.226493  root_mean_squared_error       0.574245    6.947471                0.574245           6.947471            1       True         15\n",
      "20          CatBoost_r69_BAG_L1 -25289.475966  root_mean_squared_error       0.084461    8.370067                0.084461           8.370067            1       True         33\n",
      "21    NeuralNetTorch_r86_BAG_L1 -25304.091702  root_mean_squared_error       0.206016   18.492421                0.206016          18.492421            1       True         28\n",
      "22            LightGBMXT_BAG_L2 -25384.042640  root_mean_squared_error       2.584430  257.599694                0.232295          10.883055            2       True         40\n",
      "23           CatBoost_r9_BAG_L1 -25515.834127  root_mean_squared_error       0.091614   44.639567                0.091614          44.639567            1       True         14\n",
      "24          CatBoost_r50_BAG_L1 -25561.003602  root_mean_squared_error       0.089208   12.944772                0.089208          12.944772            1       True         29\n",
      "25         LightGBM_r130_BAG_L1 -25588.421361  root_mean_squared_error       0.246761    8.858992                0.246761           8.858992            1       True         27\n",
      "26           XGBoost_r89_BAG_L1 -25588.949438  root_mean_squared_error       0.102609    5.053333                0.102609           5.053333            1       True         25\n",
      "27              LightGBM_BAG_L2 -25794.361787  root_mean_squared_error       2.412095  248.915536                0.059961           2.198898            2       True         41\n",
      "28          LightGBM_r96_BAG_L2 -25824.829194  root_mean_squared_error       2.794308  255.643673                0.442174           8.927034            2       True         54\n",
      "29         LightGBM_r188_BAG_L2 -25940.275999  root_mean_squared_error       2.457723  251.504283                0.105589           4.787645            2       True         62\n",
      "30        NeuralNetTorch_BAG_L2 -26070.550515  root_mean_squared_error       2.579719  268.006874                0.227585          21.290235            2       True         47\n",
      "31         LightGBM_r130_BAG_L2 -26255.493033  root_mean_squared_error       2.451315  250.251701                0.099181           3.535063            2       True         64\n",
      "32         LightGBM_r161_BAG_L1 -26308.880896  root_mean_squared_error       0.235080    6.144513                0.235080           6.144513            1       True         35\n",
      "33    NeuralNetTorch_r22_BAG_L1 -26381.185152  root_mean_squared_error       0.195084   32.135263                0.195084          32.135263            1       True         16\n",
      "34         LightGBMLarge_BAG_L1 -26531.039273  root_mean_squared_error       0.279249   21.211214                0.279249          21.211214            1       True          9\n",
      "35              LightGBM_BAG_L1 -26574.561575  root_mean_squared_error       0.165141   11.131428                0.165141          11.131428            1       True          2\n",
      "36          XGBoost_r194_BAG_L1 -26693.903756  root_mean_squared_error       0.098830    6.616899                0.098830           6.616899            1       True         31\n",
      "37               XGBoost_BAG_L1 -26701.215075  root_mean_squared_error       0.084029    4.793285                0.084029           4.793285            1       True          7\n",
      "38         LightGBM_r131_BAG_L1 -26715.484608  root_mean_squared_error       0.325974   16.289741                0.325974          16.289741            1       True         12\n",
      "39        NeuralNetTorch_BAG_L1 -26734.620507  root_mean_squared_error       0.191964   23.410396                0.191964          23.410396            1       True          8\n",
      "40         LightGBM_r196_BAG_L1 -26987.311918  root_mean_squared_error       0.196433    2.498890                0.196433           2.498890            1       True         37\n",
      "41    NeuralNetTorch_r22_BAG_L2 -27129.949202  root_mean_squared_error       2.606632  264.230066                0.254498          17.513428            2       True         55\n",
      "42           XGBoost_r33_BAG_L1 -27269.752973  root_mean_squared_error       0.420594   35.672115                0.420594          35.672115            1       True         17\n",
      "43          CatBoost_r13_BAG_L1 -27372.123704  root_mean_squared_error       0.087898   25.305891                0.087898          25.305891            1       True         21\n",
      "44    NeuralNetTorch_r79_BAG_L2 -27654.503305  root_mean_squared_error       2.595007  271.926347                0.242873          25.209709            2       True         50\n",
      "45         ExtraTreesMSE_BAG_L1 -27854.531942  root_mean_squared_error       0.163366    0.533503                0.163366           0.533503            1       True          5\n",
      "46       NeuralNetFastAI_BAG_L2 -27959.841892  root_mean_squared_error       2.467788  260.291365                0.115654          13.574727            2       True         45\n",
      "47        ExtraTrees_r42_BAG_L1 -28004.013193  root_mean_squared_error       0.115604    0.496912                0.115604           0.496912            1       True         18\n",
      "48    NeuralNetTorch_r79_BAG_L1 -28344.655343  root_mean_squared_error       0.157090   19.342854                0.157090          19.342854            1       True         11\n",
      "49           CatBoost_r9_BAG_L2 -28389.282732  root_mean_squared_error       2.448753  263.750110                0.096619          17.033472            2       True         53\n",
      "50    NeuralNetTorch_r30_BAG_L1 -28503.775777  root_mean_squared_error       0.200182   22.167001                0.200182          22.167001            1       True         26\n",
      "51   NeuralNetFastAI_r11_BAG_L1 -28513.620028  root_mean_squared_error       0.165868   15.946890                0.165868          15.946890            1       True         30\n",
      "52  NeuralNetFastAI_r191_BAG_L2 -28651.920954  root_mean_squared_error       2.505774  262.966045                0.153639          16.249407            2       True         52\n",
      "53       RandomForestMSE_BAG_L1 -28704.794477  root_mean_squared_error       0.127888    0.798867                0.127888           0.798867            1       True          3\n",
      "54       ExtraTrees_r172_BAG_L1 -28773.206687  root_mean_squared_error       0.127924    0.445583                0.127924           0.445583            1       True         32\n",
      "55     RandomForest_r195_BAG_L1 -28823.593651  root_mean_squared_error       0.202633    0.794796                0.202633           0.794796            1       True         22\n",
      "56      RandomForest_r39_BAG_L1 -28892.602720  root_mean_squared_error       0.164306    0.691465                0.164306           0.691465            1       True         38\n",
      "57  NeuralNetFastAI_r102_BAG_L2 -29995.119194  root_mean_squared_error       2.599545  260.275741                0.247411          13.559103            2       True         59\n",
      "58  NeuralNetFastAI_r102_BAG_L1 -30111.194318  root_mean_squared_error       0.219218   22.277481                0.219218          22.277481            1       True         20\n",
      "59  NeuralNetFastAI_r191_BAG_L1 -30540.424875  root_mean_squared_error       0.133331   15.658646                0.133331          15.658646            1       True         13\n",
      "60       NeuralNetFastAI_BAG_L1 -30574.133921  root_mean_squared_error       0.105544   12.730811                0.105544          12.730811            1       True          6\n",
      "61  NeuralNetFastAI_r145_BAG_L1 -30875.930091  root_mean_squared_error       0.147871   15.166227                0.147871          15.166227            1       True         24\n",
      "62  NeuralNetFastAI_r103_BAG_L1 -39358.883609  root_mean_squared_error       0.120565   12.051271                0.120565          12.051271            1       True         34\n",
      "63          CatBoost_r13_BAG_L2 -48160.589075  root_mean_squared_error       2.444163  253.651134                0.092028           6.934496            2       True         60\n",
      "64          CatBoost_r70_BAG_L1 -49585.179469  root_mean_squared_error       0.085762    3.102619                0.085762           3.102619            1       True         36\n",
      "Fichier submission_1_CatBoost_r70_BAG_L1.csv généré !\n",
      "Fichier submission_2_CatBoost_r13_BAG_L2.csv généré !\n",
      "Fichier submission_3_NeuralNetFastAI_r103_BAG_L1.csv généré !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_data = X_test.copy()\n",
    "train_data = X_train.copy()\n",
    "train_data['SalePrice'] = y_train\n",
    "\n",
    "label = \"SalePrice\"\n",
    "\n",
    "# Création du dataset AG\n",
    "train_ag = TabularDataset(train_data)\n",
    "\n",
    "# Création et entraînement du prédicteur\n",
    "predictor = TabularPredictor(\n",
    "        label=label,\n",
    "        eval_metric=\"root_mean_squared_error\",\n",
    "        verbosity=2\n",
    "    ).fit(\n",
    "        train_data=train_ag,\n",
    "        time_limit=1500,   # temps max en secondes\n",
    "        presets=\"best_quality\",  # meilleure qualité\n",
    "        num_cpus=1,       # pas de Ray\n",
    "        num_gpus=0\n",
    "    )\n",
    "\n",
    "# Leaderboard complet\n",
    "lb = predictor.leaderboard(silent=False)\n",
    "\n",
    "# On garde les 3 meilleurs modèles\n",
    "top3_models = lb.sort_values(\"score_val\").head(3)[\"model\"].tolist()\n",
    "\n",
    "# Test dataset en format AG\n",
    "test_ag = TabularDataset(test_data)\n",
    "\n",
    "# Génération des prédictions pour chaque modèle du top 3\n",
    "for rank, model_name in enumerate(top3_models, start=1):\n",
    "    preds = predictor.predict(test_ag, model=model_name)\n",
    "    \n",
    "    # Sauvegarde avec nom dynamique\n",
    "    filename = f\"submission_{rank}_{model_name}.csv\"\n",
    "    submission = pd.DataFrame({\n",
    "        \"Id\": test_ids,\n",
    "        \"SalePrice\": preds\n",
    "    })\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"Fichier {filename} généré !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4aca86",
   "metadata": {},
   "source": [
    "#  Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02c255ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:  # mettre à True pour activer l'optimisation\n",
    "    def objective(trial):\n",
    "        # Choix du modèle\n",
    "        model_name = trial.suggest_categorical(\"model\", [\"xgb\", \"lgbm\", \"gbr\"])\n",
    "        \n",
    "        if model_name == \"xgb\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1500),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
    "                \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "                \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-8, 10.0),\n",
    "                \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-8, 10.0),\n",
    "                \"random_state\": 42,\n",
    "                \"n_jobs\": 4,\n",
    "            }\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "        \n",
    "        elif model_name == \"lgbm\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1500),\n",
    "                \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 100),\n",
    "                \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
    "                \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "                \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-8, 10.0),\n",
    "                \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-8, 10.0),\n",
    "                \"random_state\": 42,\n",
    "                \"n_jobs\": 4\n",
    "            }\n",
    "            model = lgb.LGBMRegressor(**params)\n",
    "        \n",
    "        elif model_name == \"rf\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 2000),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 5, 50),\n",
    "                \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "                \"random_state\": 42,\n",
    "                \"n_jobs\": 4\n",
    "            }\n",
    "            model = RandomForestRegressor(**params)\n",
    "        \n",
    "        elif model_name == \"gbr\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1500),\n",
    "                \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
    "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "                \"random_state\": 42\n",
    "            }\n",
    "            model = GradientBoostingRegressor(**params)\n",
    "        \n",
    "        # Pipeline avec preprocessing\n",
    "        pipeline = Pipeline(steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "        \n",
    "        # Validation croisée\n",
    "        cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(\n",
    "            pipeline, X_train, y_train,\n",
    "            cv=cv, scoring=\"neg_root_mean_squared_error\", n_jobs=4\n",
    "        )\n",
    "        \n",
    "        rmse = -scores.mean()\n",
    "        return rmse\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1)\n",
    "        )  # on veut minimiser le RMSE\n",
    "    study.optimize(objective, n_trials=500, timeout=1500, n_jobs=2)  # 500 essais, en parallèle, 1500 secondes max\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    print(\"  RMSE:\", study.best_value)\n",
    "    print(\"  Params:\", study.best_params)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_model_name = best_params[\"model\"]\n",
    "\n",
    "    # enlever la clé \"model\"\n",
    "    model_params = {k: v for k, v in best_params.items() if k != \"model\"}\n",
    "\n",
    "    if best_model_name == \"xgb\":\n",
    "        model = xgb.XGBRegressor(**model_params)\n",
    "    elif best_model_name == \"lgbm\":\n",
    "        model = lgb.LGBMRegressor(**model_params)\n",
    "    elif best_model_name == \"rf\":\n",
    "        model = RandomForestRegressor(**model_params)\n",
    "    elif best_model_name == \"gbr\":\n",
    "        model = GradientBoostingRegressor(**model_params)\n",
    "\n",
    "    best_model = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    # fit + prédiction\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        \"Id\": test_ids,  \n",
    "        \"SalePrice\": y_pred\n",
    "    })\n",
    "    submission.to_csv(\"submission_optuna.csv\", index=False)\n",
    "\n",
    "    print(\"Submission saved as submission_optuna.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
