{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1edf55c",
   "metadata": {},
   "source": [
    "# House "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e364d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ydata_profiling import ProfileReport\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import os\n",
    "\n",
    "# Désactive les erreurs Ray parasites\n",
    "os.environ[\"RAY_IGNORE_UNHANDLED_ERRORS\"] = \"1\"\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "test_ids = test['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23c5aea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'LotShape',\n",
      "       'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1',\n",
      "       'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'RoofStyle',\n",
      "       'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'ExterQual',\n",
      "       'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
      "       'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
      "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
      "       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
      "       'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n",
      "       'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageArea',\n",
      "       'GarageQual', 'GarageCond', 'PavedDrive', 'MoSold', 'YrSold',\n",
      "       'SaleType', 'SaleCondition', 'Age_Built', 'Age_RemodAdd',\n",
      "       'HasBsmtFinSF2', 'HasBsmtFinSF1', 'HeatingQC_num', 'Has2ndFlr',\n",
      "       'HasLowQualFinSF', 'GarageAge', 'HasGarage', 'GarageArea_log',\n",
      "       'HasWoodDeck', 'WoodDeckSF_log', 'HasOpenPorch', 'OpenPorchSF_log',\n",
      "       'HasScreenPorch', 'ScreenPorchLog', 'GotPool', 'Fence_wo', 'Fence_Prv',\n",
      "       'MoSold_sin', 'MoSold_cos'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "y= train[\"SalePrice\"]\n",
    "train = train.drop(columns=[\"SalePrice\",\"Id\"])\n",
    "test = test.drop(columns=[\"Id\"])\n",
    "\n",
    "# MSZoning - replace NaN with mode (RL)\n",
    "train[\"MSZoning\"] = train[\"MSZoning\"].replace(np.nan, \"RL\")\n",
    "test[\"MSZoning\"] = test[\"MSZoning\"].replace(np.nan, \"RL\")\n",
    "\n",
    "\n",
    "# LotFrontage - replace NaN with median of neighborhood + log transform\n",
    "train[\"LotFrontage\"] = train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "test[\"LotFrontage\"] = test.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "train[\"LotFrontage\"] = np.log1p(train[\"LotFrontage\"])  # log transform for skew\n",
    "test[\"LotFrontage\"] = np.log1p(test[\"LotFrontage\"])  # log transform for skew\n",
    "\n",
    "# Age_Built and Age_RemodAdd - create new features\n",
    "train[\"Age_Built\"] = train[\"YrSold\"] - train[\"YearBuilt\"]\n",
    "train[\"Age_RemodAdd\"] = train[\"YrSold\"] - train[\"YearRemodAdd\"]\n",
    "test[\"Age_Built\"] = test[\"YrSold\"] - test[\"YearBuilt\"]\n",
    "test[\"Age_RemodAdd\"] = test[\"YrSold\"] - test[\"YearRemodAdd\"]\n",
    "\n",
    "# LotArea - use log transformation to reduce skewness\n",
    "train[\"LotArea\"] = np.log1p(train[\"LotArea\"])\n",
    "test[\"LotArea\"] = np.log1p(test[\"LotArea\"])\n",
    "\n",
    "\n",
    "\n",
    "# MasVnrType - replace NaN with None\n",
    "train[\"MasVnrType\"] = train[\"MasVnrType\"].replace(np.nan, \"None\")\n",
    "test[\"MasVnrType\"] = test[\"MasVnrType\"].replace(np.nan, \"None\")\n",
    "\n",
    "# MasVnrArea - replace NaN with 0 + log transform\n",
    "train[\"MasVnrArea\"] = train[\"MasVnrArea\"].replace(np.nan, 0)\n",
    "test[\"MasVnrArea\"] = test[\"MasVnrArea\"].replace(np.nan, 0)\n",
    "train[\"MasVnrArea\"] = np.log1p(train[\"MasVnrArea\"])  # log transform for skew\n",
    "test[\"MasVnrArea\"] = np.log1p(test[\"MasVnrArea\"])  # log transform for skew\n",
    "\n",
    "# ExterCond - mapping\n",
    "mapping = {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "train[\"ExterCond\"] = train[\"ExterCond\"].map(mapping)\n",
    "test[\"ExterCond\"] = test[\"ExterCond\"].map(mapping)\n",
    "# ExterQual - mapping\n",
    "mapping = {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "train[\"ExterQual\"] = train[\"ExterQual\"].map(mapping)\n",
    "test[\"ExterQual\"] = test[\"ExterQual\"].map(mapping)\n",
    "\n",
    "#BstmQual - replace NaN with None + mapping\n",
    "train[\"BsmtQual\"] = train[\"BsmtQual\"].replace(np.nan, \"None\")\n",
    "test[\"BsmtQual\"] = test[\"BsmtQual\"].replace(np.nan, \"None\")\n",
    "mapping = {\"None\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "train[\"BsmtQual\"] = train[\"BsmtQual\"].map(mapping)\n",
    "test[\"BsmtQual\"] = test[\"BsmtQual\"].map(mapping)\n",
    "\n",
    "# BsmtCOnd - replace NaN with None + mapping\n",
    "train[\"BsmtCond\"] = train[\"BsmtCond\"].replace(np.nan, \"None\")\n",
    "test[\"BsmtCond\"] = test[\"BsmtCond\"].replace(np.nan, \"None\")\n",
    "mapping = {\"None\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "train[\"BsmtCond\"] = train[\"BsmtCond\"].map(mapping)\n",
    "test[\"BsmtCond\"] = test[\"BsmtCond\"].map(mapping)\n",
    "\n",
    "# BsmtHalfBath - replace NaN with 0\n",
    "train[\"BsmtHalfBath\"] = train[\"BsmtHalfBath\"].replace(np.nan, 0)\n",
    "test[\"BsmtHalfBath\"] = test[\"BsmtHalfBath\"].replace(np.nan, 0)\n",
    "\n",
    "# BsmtFullBath - replace NaN with 0\n",
    "train[\"BsmtFullBath\"] = train[\"BsmtFullBath\"].replace(np.nan, 0)\n",
    "test[\"BsmtFullBath\"] = test[\"BsmtFullBath\"].replace(np.nan, 0)\n",
    "\n",
    "# BsmtExposure - replace NaN with None + mapping\n",
    "train[\"BsmtExposure\"] = train[\"BsmtExposure\"].replace(np.nan, \"None\")\n",
    "test[\"BsmtExposure\"] = test[\"BsmtExposure\"].replace(np.nan, \"None\")\n",
    "mapping = {\"None\": 0, \"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4}\n",
    "train[\"BsmtExposure\"] = train[\"BsmtExposure\"].map(mapping)\n",
    "test[\"BsmtExposure\"] = test[\"BsmtExposure\"].map(mapping)\n",
    "\n",
    "#BsmtfinType1 - replace NaN with None + mapping\n",
    "train[\"BsmtFinType1\"] = train[\"BsmtFinType1\"].replace(np.nan, \"None\")\n",
    "test[\"BsmtFinType1\"] = test[\"BsmtFinType1\"].replace(np.nan, \"None\")\n",
    "mapping = {\"None\": 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\n",
    "train[\"BsmtFinType1\"] = train[\"BsmtFinType1\"].map(mapping)\n",
    "test[\"BsmtFinType1\"] = test[\"BsmtFinType1\"].map(mapping)\n",
    "\n",
    "# BsmtFinSF2 - replace NaN with 0 + log transform + binary indicator\n",
    "train[\"BsmtFinSF2\"] = train[\"BsmtFinSF2\"].replace(np.nan, 0)\n",
    "test[\"BsmtFinSF2\"] = test[\"BsmtFinSF2\"].replace(np.nan, 0)\n",
    "train[\"HasBsmtFinSF2\"] = (train[\"BsmtFinSF2\"] > 0).astype(int)\n",
    "test[\"HasBsmtFinSF2\"] = (test[\"BsmtFinSF2\"] > 0).astype(int)\n",
    "train[\"BsmtFinSF2\"] = np.log1p(train[\"BsmtFinSF2\"])  # log transform for skew\n",
    "test[\"BsmtFinSF2\"] = np.log1p(test[\"BsmtFinSF2\"])  # log transform for skew\n",
    "\n",
    "# BsmtFinSF1 - replace NaN with 0 + log transform + binary indicator\n",
    "train[\"BsmtFinSF1\"] = train[\"BsmtFinSF1\"].replace(np.nan, 0)\n",
    "test[\"BsmtFinSF1\"] = test[\"BsmtFinSF1\"].replace(np.nan, 0)\n",
    "train[\"HasBsmtFinSF1\"] = (train[\"BsmtFinSF1\"] > 0).astype(int)\n",
    "test[\"HasBsmtFinSF1\"] = (test[\"BsmtFinSF1\"] > 0).astype(int)\n",
    "train[\"BsmtFinSF1\"] = np.log1p(train[\"BsmtFinSF1\"])  # log transform for skew\n",
    "test[\"BsmtFinSF1\"] = np.log1p(test[\"BsmtFinSF1\"])  # log transform for skew\n",
    "\n",
    "# BsmtFinType2 - replace NaN with None + mapping\n",
    "train[\"BsmtFinType2\"] = train[\"BsmtFinType2\"].replace(np.nan, \"None\")\n",
    "test[\"BsmtFinType2\"] = test[\"BsmtFinType2\"].replace(np.nan, \"None\")\n",
    "mapping = {\"None\": 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\n",
    "train[\"BsmtFinType2\"] = train[\"BsmtFinType2\"].map(mapping)\n",
    "test[\"BsmtFinType2\"] = test[\"BsmtFinType2\"].map(mapping)\n",
    "\n",
    "# BsmtUnfSF - replace NaN with 0 + log transform\n",
    "train[\"BsmtUnfSF\"] = train[\"BsmtUnfSF\"].replace(np.nan, 0)\n",
    "test[\"BsmtUnfSF\"] = test[\"BsmtUnfSF\"].replace(np.nan, 0)\n",
    "train[\"BsmtUnfSF\"] = np.log1p(train[\"BsmtUnfSF\"])  # log transform for skew\n",
    "test[\"BsmtUnfSF\"] = np.log1p(test[\"BsmtUnfSF\"])  # log transform for skew\n",
    "\n",
    "# TotalBsmtSF - use log transformation to reduce skewness\n",
    "# replace NaN with 0 (no basement)\n",
    "train[\"TotalBsmtSF\"] = train[\"TotalBsmtSF\"].replace(np.nan, 0)\n",
    "test[\"TotalBsmtSF\"] = test[\"TotalBsmtSF\"].replace(np.nan, 0)\n",
    "train[\"TotalBsmtSF\"] = np.log1p(train[\"TotalBsmtSF\"])\n",
    "test[\"TotalBsmtSF\"] = np.log1p(test[\"TotalBsmtSF\"])\n",
    "\n",
    "# HeatingQC \n",
    "mapping = {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "train[\"HeatingQC_num\"] = train[\"HeatingQC\"].map(mapping)\n",
    "test[\"HeatingQC_num\"] = test[\"HeatingQC\"].map(mapping)\n",
    "\n",
    "# Electrical - replace NaN with mode (SBrkr)\n",
    "train[\"Electrical\"] = train[\"Electrical\"].replace(np.nan, \"SBrkr\")\n",
    "test[\"Electrical\"] = test[\"Electrical\"].replace(np.nan, \"SBrkr\")\n",
    "\n",
    "# Exterior1st - replace NaN with mode (VinylSd)\n",
    "train[\"Exterior1st\"] = train[\"Exterior1st\"].replace(np.nan, \"VinylSd\")\n",
    "test[\"Exterior1st\"] = test[\"Exterior1st\"].replace(np.nan, \"VinylSd\")\n",
    "\n",
    "# Exterior2nd - replace NaN with mode (VinylSd)\n",
    "train[\"Exterior2nd\"] = train[\"Exterior2nd\"].replace(np.nan, \"VinylSd\")\n",
    "test[\"Exterior2nd\"] = test[\"Exterior2nd\"].replace(np.nan, \"VinylSd\")\n",
    "\n",
    "# First Floor SF - use log transformation to reduce skewness\n",
    "train[\"1stFlrSF\"] = np.log1p(train[\"1stFlrSF\"])\n",
    "test[\"1stFlrSF\"] = np.log1p(test[\"1stFlrSF\"])\n",
    "\n",
    "# 2nd Flr SF - create binary indicator + log transform\n",
    "train[\"Has2ndFlr\"] = (train[\"2ndFlrSF\"] > 0).astype(int)\n",
    "test[\"Has2ndFlr\"] = (test[\"2ndFlrSF\"] > 0).astype(int)\n",
    "train[\"2ndFlrSF\"] = np.log1p(train[\"2ndFlrSF\"])  # log transform for skew\n",
    "test[\"2ndFlrSF\"] = np.log1p(test[\"2ndFlrSF\"])  # log transform for skew\n",
    "\n",
    "# Low Qualfin SF - binomial indicator\n",
    "train[\"HasLowQualFinSF\"] = (train[\"LowQualFinSF\"] > 0).astype(int)\n",
    "test[\"HasLowQualFinSF\"] = (test[\"LowQualFinSF\"] > 0).astype(int)\n",
    "\n",
    "# GrLivArea - use log transformation to reduce skewness\n",
    "train[\"GrLivArea\"] = np.log1p(train[\"GrLivArea\"])\n",
    "test[\"GrLivArea\"] = np.log1p(test[\"GrLivArea\"])\n",
    "\n",
    "# Kitchen Qual - replace NaN with mode (TA) + mapping\n",
    "mapping = {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "train[\"KitchenQual\"] = train[\"KitchenQual\"].replace(np.nan, \"TA\")\n",
    "test[\"KitchenQual\"] = test[\"KitchenQual\"].replace(np.nan, \"TA\")\n",
    "train[\"KitchenQual\"] = train[\"KitchenQual\"].map(mapping)\n",
    "test[\"KitchenQual\"] = test[\"KitchenQual\"].map(mapping)\n",
    "\n",
    "#FireplaceQu - replace NaN with None + mapping\n",
    "mapping = {\"None\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}  \n",
    "train[\"FireplaceQu\"] = train[\"FireplaceQu\"].replace(np.nan, \"None\")\n",
    "test[\"FireplaceQu\"] = test[\"FireplaceQu\"].replace(np.nan, \"None\")\n",
    "train[\"FireplaceQu\"] = train[\"FireplaceQu\"].map(mapping)\n",
    "test[\"FireplaceQu\"] = test[\"FireplaceQu\"].map(mapping)\n",
    "\n",
    "# GarageYrBlt - create new feature GarageAge + replace NaN with maxvalue \n",
    "max_year = max(train[\"YearBuilt\"].max(), train[\"YearRemodAdd\"].max(), train[\"YrSold\"].max()) + 1\n",
    "train[\"GarageYrBlt\"] = train[\"GarageYrBlt\"].replace(np.nan, max_year)\n",
    "test[\"GarageYrBlt\"] = test[\"GarageYrBlt\"].replace(np.nan, max_year)\n",
    "train[\"GarageAge\"] = train[\"YrSold\"] - train[\"GarageYrBlt\"]\n",
    "test[\"GarageAge\"] = test[\"YrSold\"] - test[\"GarageYrBlt\"]\n",
    "\n",
    "# GarageFinish - replace NaN with None + mapping\n",
    "train[\"GarageFinish\"] = train[\"GarageFinish\"].replace(np.nan, \"None\")\n",
    "test[\"GarageFinish\"] = test[\"GarageFinish\"].replace(np.nan, \"None\")\n",
    "mapping = {\"Fin\": 1, \"RFn\": 2, \"Unf\": 3, \"None\": 0}\n",
    "train[\"GarageFinish\"] = train[\"GarageFinish\"].map(mapping)\n",
    "test[\"GarageFinish\"] = test[\"GarageFinish\"].map(mapping)\n",
    "\n",
    "# GarageCond - mapping + replace NaN with None\n",
    "mapping = {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5, \"None\": 0}\n",
    "train[\"GarageCond\"] = train[\"GarageCond\"].replace(np.nan, \"None\")\n",
    "test[\"GarageCond\"] = test[\"GarageCond\"].replace(np.nan, \"None\")\n",
    "train[\"GarageCond\"] = train[\"GarageCond\"].map(mapping)\n",
    "test[\"GarageCond\"] = test[\"GarageCond\"].map(mapping)\n",
    "\n",
    "# GarageArea - create binary indicator and log transform + replace NaN with 0\n",
    "train[\"GarageArea\"] = train[\"GarageArea\"].fillna(0)\n",
    "test[\"GarageArea\"] = test[\"GarageArea\"].fillna(0)\n",
    "train[\"HasGarage\"] = (train[\"GarageArea\"] > 0).astype(int)\n",
    "test[\"HasGarage\"] = (test[\"GarageArea\"] > 0).astype(int)\n",
    "train[\"GarageArea_log\"] = np.log1p(train[\"GarageArea\"])  # log transform for skew\n",
    "test[\"GarageArea_log\"] = np.log1p(test[\"GarageArea\"])\n",
    "\n",
    "# GarageType - replace NaN with None\n",
    "train[\"GarageType\"] = train[\"GarageType\"].replace(np.nan, \"None\")\n",
    "test[\"GarageType\"] = test[\"GarageType\"].replace(np.nan, \"None\")\n",
    "\n",
    "# GarageQual - replace NaN with None + mapping\n",
    "mapping = {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5, \"None\": 0}\n",
    "train[\"GarageQual\"] = train[\"GarageQual\"].replace(np.nan, \"None\")\n",
    "test[\"GarageQual\"] = test[\"GarageQual\"].replace(np.nan, \"None\")\n",
    "train[\"GarageQual\"] = train[\"GarageQual\"].map(mapping)\n",
    "test[\"GarageQual\"] = test[\"GarageQual\"].map(mapping)\n",
    "\n",
    "#Same with WoodDeckSF\n",
    "train[\"HasWoodDeck\"] = (train[\"WoodDeckSF\"] > 0).astype(int)\n",
    "test[\"HasWoodDeck\"] = (test[\"WoodDeckSF\"] > 0).astype(int)\n",
    "train[\"WoodDeckSF_log\"] = np.log1p(train[\"WoodDeckSF\"])  # log transform for skew\n",
    "test[\"WoodDeckSF_log\"] = np.log1p(test[\"WoodDeckSF\"])\n",
    "\n",
    "# Binary indicator\n",
    "train[\"HasOpenPorch\"] = (train[\"OpenPorchSF\"] > 0).astype(int)\n",
    "test[\"HasOpenPorch\"] = (test[\"OpenPorchSF\"] > 0).astype(int)\n",
    "\n",
    "# Optionally keep raw or transform it\n",
    "train[\"OpenPorchSF_log\"] = np.log1p(train[\"OpenPorchSF\"])  # log transform for skew\n",
    "test[\"OpenPorchSF_log\"] = np.log1p(test[\"OpenPorchSF\"])\n",
    "\n",
    "train[\"HasScreenPorch\"] = (train[\"ScreenPorch\"] > 0).astype(int)\n",
    "train[\"ScreenPorchLog\"] = np.log1p(train[\"ScreenPorch\"])\n",
    "test[\"HasScreenPorch\"] = (test[\"ScreenPorch\"] > 0).astype(int)\n",
    "test[\"ScreenPorchLog\"] = np.log1p(test[\"ScreenPorch\"])\n",
    "\n",
    "train[\"GotPool\"] = train[\"PoolQC\"].notnull().astype(int)\n",
    "test[\"GotPool\"] = test[\"PoolQC\"].notnull().astype(int)\n",
    "\n",
    "# Fence - replace NaN with None + mapping with quality and good wood\n",
    "# Fence - mapping with quality and good wood\n",
    "# Fence_wo: 2 if GdWo, 1 if MnWw, else 0\n",
    "# Fence_Prv: 2 if GdPrv, 1 if MnPrv, else 0\n",
    "def fence_wo(val):\n",
    "    if val == \"GdWo\":\n",
    "        return 2\n",
    "    elif val == \"MnWw\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def fence_prv(val):\n",
    "    if val == \"GdPrv\":\n",
    "        return 2\n",
    "    elif val == \"MnPrv\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "train[\"Fence_wo\"] = train[\"Fence\"].apply(fence_wo)\n",
    "test[\"Fence_wo\"] = test[\"Fence\"].apply(fence_wo)\n",
    "train[\"Fence_Prv\"] = train[\"Fence\"].apply(fence_prv)\n",
    "test[\"Fence_Prv\"] = test[\"Fence\"].apply(fence_prv)\n",
    "\n",
    "mapping = {\"GdWo\": 2, \"MnPrv\": 1, \"GdPrv\": 2, \"MnWw\": 1, \"None\": 0}\n",
    "train[\"Fence\"] = train[\"Fence\"].map(mapping)\n",
    "test[\"Fence\"] = test[\"Fence\"].map(mapping)\n",
    "\n",
    "train[\"MoSold_sin\"] = np.sin(2 * np.pi * train[\"MoSold\"] / 12)\n",
    "train[\"MoSold_cos\"] = np.cos(2 * np.pi * train[\"MoSold\"] / 12)\n",
    "test[\"MoSold_sin\"] = np.sin(2 * np.pi * test[\"MoSold\"] / 12)\n",
    "test[\"MoSold_cos\"] = np.cos(2 * np.pi * test[\"MoSold\"] / 12)\n",
    "\n",
    "# Sale type - replace NaN with mode (WD)\n",
    "train[\"SaleType\"] = train[\"SaleType\"].replace(np.nan, \"WD\")\n",
    "test[\"SaleType\"] = test[\"SaleType\"].replace(np.nan, \"WD\")\n",
    "\n",
    "train = train.drop(columns=[\"BsmtFinType2\",\"ExterCond\",\"PoolArea\",\"OpenPorchSF\",\"WoodDeckSF\",\"LowQualFinSF\",\"YearRemodAdd\",\"YearBuilt\",\"GarageYrBlt\",\"PoolQC\",\"Fence\",\"Functional\",\"GarageCars\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\",\"MiscFeature\",\"MiscVal\",\"RoofMatl\",\"Condition2\",\"Alley\", \"Street\", \"Utilities\", \"MiscFeature\"])\n",
    "test = test.drop(columns=[\"BsmtFinType2\",\"ExterCond\",\"PoolArea\",\"OpenPorchSF\",\"WoodDeckSF\",\"LowQualFinSF\",\"YearRemodAdd\",\"YearBuilt\",\"GarageYrBlt\",\"PoolQC\",\"Fence\",\"Functional\",\"GarageCars\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\",\"MiscFeature\",\"MiscVal\",\"RoofMatl\",\"Condition2\",\"Alley\", \"Street\", \"Utilities\", \"MiscFeature\"])\n",
    "\n",
    "print(test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b62d7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#je garde à contre coeur : LandSlope, Screeporch PavedDrive?\n",
    "# truc potentiellement intéréssant : MiscVal kitchenAbove\n",
    "# truc ou je me suis permis des dinguerie : PoolQC (et donc poolarea) \n",
    "num_cols = [\"Fence_wo\",\"Fence_Prv\",\"GarageQual\",\"KitchenQual\",\"BsmtFinType1\",\"BsmtFinSF2\",\"MasVnrArea\",\"ExterQual\",\"BsmtQual\",\"BsmtCond\",\"FireplaceQu\",\"GarageFinish\",\"BsmtExposure\",\"BsmtFinSF1\",\"BsmtUnfSF\",\"TotalBsmtSF\",\"GarageCond\",\"HeatingQC_num\",\"1stFlrSF\",\"2ndFlrSF\",\"GrLivArea\",\"BsmtFullBath\",\"BsmtHalfBath\",\"FullBath\",\"HalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"TotRmsAbvGrd\",\"Fireplaces\",\"GarageAge\",\"GarageArea_log\",\"WoodDeckSF_log\",\"HasOpenPorch\",\"HasScreenPorch\",\"MoSold_sin\",\"MoSold_cos\",\"Age_Built\",\"Age_RemodAdd\",\"LotFrontage\",\"LotArea\",\"OverallQual\",\"OverallCond\"]\n",
    "cat_cols = [\"MasVnrType\",\"Foundation\",\"HasBsmtFinSF1\",\"Heating\",\"CentralAir\",\"Electrical\",\"HasBsmtFinSF2\",\"Has2ndFlr\",\"HasLowQualFinSF\",\"GarageType\",\"HasGarage\",\"PavedDrive\",\"HasWoodDeck\",\"HasOpenPorch\",\"HasScreenPorch\",\"GotPool\",\"YrSold\",\"SaleType\",\"SaleCondition\",\"Exterior1st\",\"Exterior2nd\",\"RoofStyle\",\"MSSubClass\",\"HouseStyle\",\"Neighborhood\",\"BldgType\",\"MSSubClass\",\"LotShape\",\"LandContour\",\"LotConfig\",\"LandSlope\",\"Condition1\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # One-hot categorical features\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        # Pass-through numerical features\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"  # drop unused raw columns like Name, Ticket, Cabin, etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e93c6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train columns: ['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'RoofStyle', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'ExterQual', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'Age_Built', 'Age_RemodAdd', 'HasBsmtFinSF2', 'HasBsmtFinSF1', 'HeatingQC_num', 'Has2ndFlr', 'HasLowQualFinSF', 'GarageAge', 'HasGarage', 'GarageArea_log', 'HasWoodDeck', 'WoodDeckSF_log', 'HasOpenPorch', 'OpenPorchSF_log', 'HasScreenPorch', 'ScreenPorchLog', 'GotPool', 'Fence_wo', 'Fence_Prv', 'MoSold_sin', 'MoSold_cos']\n",
      "X_test columns: ['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'RoofStyle', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'ExterQual', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'Age_Built', 'Age_RemodAdd', 'HasBsmtFinSF2', 'HasBsmtFinSF1', 'HeatingQC_num', 'Has2ndFlr', 'HasLowQualFinSF', 'GarageAge', 'HasGarage', 'GarageArea_log', 'HasWoodDeck', 'WoodDeckSF_log', 'HasOpenPorch', 'OpenPorchSF_log', 'HasScreenPorch', 'ScreenPorchLog', 'GotPool', 'Fence_wo', 'Fence_Prv', 'MoSold_sin', 'MoSold_cos']\n",
      "X_train shape: (1460, 77)\n",
      "y_train shape: (1460,)\n",
      "X_test shape: (1459, 77)\n",
      "Colonnes manquantes dans test: set()\n",
      "Colonnes manquantes dans train: set()\n",
      "RMSE par fold: [28579.62609796 26994.73622467 45406.76442523 27797.42670591\n",
      " 24333.11081144]\n",
      "RMSE moyen: 30622.332853041986\n"
     ]
    }
   ],
   "source": [
    "X_train = train\n",
    "X_train2= X_train.copy()\n",
    "y_train = y\n",
    "y_train2 = y_train.copy()\n",
    "X_test = test\n",
    "X_test2 = X_test.copy()\n",
    "\n",
    "\n",
    "print(\"X_train columns:\", X_train.columns.tolist())\n",
    "print(\"X_test columns:\", X_test.columns.tolist())\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "missing_in_test = set(X_train.columns) - set(X_test.columns)\n",
    "missing_in_train = set(X_test.columns) - set(X_train.columns)\n",
    "print(\"Colonnes manquantes dans test:\", missing_in_test)\n",
    "print(\"Colonnes manquantes dans train:\", missing_in_train)\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=500, max_depth=None, random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=3, random_state=42),\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"Lasso\": Lasso(alpha=0.0005, max_iter=5000),\n",
    "    \"SVR\": SVR(kernel=\"rbf\", C=20, epsilon=0.1),\n",
    "    \"KNN\": KNeighborsRegressor(n_neighbors=5),\n",
    "    \"XGB\": xgb.XGBRegressor(n_estimators=2000, learning_rate=0.05, max_depth=4, subsample=0.7, colsample_bytree=0.7, random_state=42),\n",
    "    \"LGBM\": lgb.LGBMRegressor(n_estimators=2000, learning_rate=0.05, max_depth=-1, subsample=0.7, colsample_bytree=0.7, random_state=42),\n",
    "}\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", model)\n",
    "])\n",
    "\n",
    "# 5-fold CV\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scores = cross_val_score(\n",
    "    pipeline, X_train, y_train,\n",
    "    cv=cv, scoring=\"neg_root_mean_squared_error\"\n",
    ")\n",
    "\n",
    "print(\"RMSE par fold:\", -scores)\n",
    "print(\"RMSE moyen:\", -np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37b0081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "257a8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"SalePrice\": y_pred\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d380cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest: RMSE = 30455.9508 ± 7513.0722\n",
      "Prédictions sauvegardées dans submission_RandomForest.csv\n",
      "GradientBoosting: RMSE = 29858.8303 ± 9892.7035\n",
      "Prédictions sauvegardées dans submission_GradientBoosting.csv\n",
      "Ridge: RMSE = 34174.6778 ± 5466.3735\n",
      "Prédictions sauvegardées dans submission_Ridge.csv\n",
      "Lasso: RMSE = 35033.6601 ± 5636.8305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Victor\\Desktop\\Kaggle\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.535e+11, tolerance: 9.208e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédictions sauvegardées dans submission_Lasso.csv\n",
      "SVR: RMSE = 80403.1131 ± 5321.8403\n",
      "Prédictions sauvegardées dans submission_SVR.csv\n",
      "KNN: RMSE = 36632.2109 ± 4798.5303\n",
      "Prédictions sauvegardées dans submission_KNN.csv\n",
      "XGB: RMSE = 28627.7023 ± 10045.3755\n",
      "Prédictions sauvegardées dans submission_XGB.csv\n",
      "LGBM: RMSE = 31645.9249 ± 8553.8557\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3282\n",
      "[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 180921.195890\n",
      "Prédictions sauvegardées dans submission_LGBM.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Victor\\Desktop\\Kaggle\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        pipeline = Pipeline(steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "        \n",
    "        # --- Cross-validation ---\n",
    "        scores = cross_val_score(\n",
    "            pipeline, X_train2, y_train2,\n",
    "            cv=cv, scoring=\"neg_root_mean_squared_error\", n_jobs=4\n",
    "        )\n",
    "        \n",
    "        rmse_scores = -scores\n",
    "        mean_rmse, std_rmse = rmse_scores.mean(), rmse_scores.std()\n",
    "        print(f\"{name}: RMSE = {mean_rmse:.4f} ± {std_rmse:.4f}\")\n",
    "        \n",
    "        # --- Train full model and predict on X_test ---\n",
    "        pipeline.fit(X_train2, y_train2)\n",
    "        y_pred = pipeline.predict(X_test2)\n",
    "        \n",
    "        submission = pd.DataFrame({\n",
    "            \"Id\": test_ids,        # Assumes you have test_ids defined\n",
    "            \"SalePrice\": y_pred\n",
    "        })\n",
    "        \n",
    "        filename = f\"submission_{name}.csv\"\n",
    "        submission.to_csv(filename, index=False)\n",
    "        print(f\"Prédictions sauvegardées dans {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9731b7bc",
   "metadata": {},
   "source": [
    "# Autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8901f408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2ndFlrSF        Has2ndFlr          0.997334\n",
      "HasScreenPorch  ScreenPorchLog     0.996544\n",
      "BsmtFinSF2      HasBsmtFinSF2      0.988754\n",
      "HasWoodDeck     WoodDeckSF_log     0.987795\n",
      "BsmtFinSF1      HasBsmtFinSF1      0.969787\n",
      "HasOpenPorch    OpenPorchSF_log    0.969147\n",
      "HasGarage       GarageArea_log     0.967719\n",
      "GarageQual      GarageCond         0.959172\n",
      "GarageCond      HasGarage          0.946245\n",
      "GarageQual      HasGarage          0.942499\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculer la matrice de corrélation pour les variables numériques\n",
    "corr_matrix = X_train.corr(numeric_only=True).abs()\n",
    "\n",
    "# On ne garde que la partie supérieure de la matrice (pour éviter les doublons)\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Trouver les paires avec la plus forte corrélation (hors diagonale)\n",
    "most_correlated = (\n",
    "    upper.stack()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "print(most_correlated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da291ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20250928_093925\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.5\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Memory Avail:       12.44 GB / 31.92 GB (39.0%)\n",
      "Disk Space Avail:   848.66 GB / 1861.39 GB (45.6%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 250s of the 1000s of remaining time (25%).\n",
      "\t\tContext path: \"c:\\Users\\Victor\\Desktop\\Kaggle\\house-prices-advanced-regression-techniques\\AutogluonModels\\ag-20250928_093925\\ds_sub_fit\\sub_fit_ho\"\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout     score_val              eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   RandomForestMSE_BAG_L2  -24744.425476 -26140.274154  root_mean_squared_error        3.588987       1.437056  65.552232                 0.122198                0.148293           1.052763            2       True         19\n",
      "1     LightGBMLarge_BAG_L2  -26297.357131 -28418.851270  root_mean_squared_error        3.724775       1.377957  68.911016                 0.257986                0.089193           4.411547            2       True         24\n",
      "2           XGBoost_BAG_L1  -26318.802923 -27864.617612  root_mean_squared_error        0.447311       0.092671   6.064089                 0.447311                0.092671           6.064089            1       True          7\n",
      "3     LightGBMLarge_BAG_L1  -27466.524759 -30426.712141  root_mean_squared_error        0.340941       0.106925   7.036252                 0.340941                0.106925           7.036252            1       True          9\n",
      "4     CatBoost_r177_BAG_L1  -27871.489913 -25981.723751  root_mean_squared_error        0.092760       0.085889   7.648750                 0.092760                0.085889           7.648750            1       True         10\n",
      "5           XGBoost_BAG_L2  -28138.666966 -26387.998124  root_mean_squared_error        3.716180       1.370697  69.843392                 0.249391                0.081933           5.343924            2       True         23\n",
      "6   NeuralNetFastAI_BAG_L2  -28744.636148 -29315.730935  root_mean_squared_error        3.736962       1.397210  75.167253                 0.270173                0.108446          10.667785            2       True         22\n",
      "7          CatBoost_BAG_L1  -28793.196666 -26170.617762  root_mean_squared_error        0.457994       0.077999  14.484020                 0.457994                0.077999          14.484020            1       True          4\n",
      "8     ExtraTreesMSE_BAG_L2  -28866.378324 -25013.492059  root_mean_squared_error        3.572038       1.466594  65.094436                 0.105249                0.177830           0.594967            2       True         21\n",
      "9   RandomForestMSE_BAG_L1  -29645.474816 -29578.437056  root_mean_squared_error        0.103173       0.121413   0.739800                 0.103173                0.121413           0.739800            1       True          3\n",
      "10     WeightedEnsemble_L3  -30205.255459 -24156.297790  root_mean_squared_error        3.829466       1.548527  70.450512                 0.008037                0.000000           0.012152            3       True         27\n",
      "11         CatBoost_BAG_L2  -30281.923962 -26490.299015  root_mean_squared_error        3.550710       1.371264  72.536510                 0.083921                0.082500           8.037041            2       True         20\n",
      "12     WeightedEnsemble_L2  -30773.164493 -24189.853224  root_mean_squared_error        2.148871       0.572550  43.098790                 0.011965                0.000000           0.011643            2       True         16\n",
      "13       LightGBMXT_BAG_L1  -30894.525961 -24956.327947  root_mean_squared_error        0.499113       0.116590   4.400611                 0.499113                0.116590           4.400611            1       True          1\n",
      "14    LightGBM_r131_BAG_L2  -31073.857494 -26551.473088  root_mean_squared_error        3.683196       1.370470  66.781099                 0.216408                0.081706           2.281631            2       True         26\n",
      "15         LightGBM_BAG_L1  -31170.917379 -26878.732820  root_mean_squared_error        0.480818       0.207039  12.354962                 0.480818                0.207039          12.354962            1       True          2\n",
      "16    LightGBM_r131_BAG_L1  -31436.527890 -26325.505423  root_mean_squared_error        0.283101       0.107799   2.993981                 0.283101                0.107799           2.993981            1       True         11\n",
      "17  NeuralNetFastAI_BAG_L1  -31948.849138 -27455.560980  root_mean_squared_error        0.884571       0.122321  11.885036                 0.884571                0.122321          11.885036            1       True          6\n",
      "18    CatBoost_r177_BAG_L2  -32181.759752 -27074.583953  root_mean_squared_error        3.552101       1.369032  67.773416                 0.085313                0.080268           3.273948            2       True         25\n",
      "19         LightGBM_BAG_L2  -32897.707114 -26653.262940  root_mean_squared_error        3.578006       1.344266  66.218735                 0.111217                0.055502           1.719267            2       True         18\n",
      "20     LightGBM_r96_BAG_L1  -33267.000071 -26470.922372  root_mean_squared_error        0.533920       0.238616   4.410202                 0.533920                0.238616           4.410202            1       True         13\n",
      "21   ExtraTrees_r42_BAG_L1  -33518.414886 -29738.276088  root_mean_squared_error        0.107062       0.157719   0.546246                 0.107062                0.157719           0.546246            1       True         15\n",
      "22    ExtraTreesMSE_BAG_L1  -34276.681090 -29531.090668  root_mean_squared_error        0.103319       0.172997   0.551502                 0.103319                0.172997           0.551502            1       True          5\n",
      "23       LightGBMXT_BAG_L2  -34523.711655 -26685.723327  root_mean_squared_error        3.587436       1.344328  66.098441                 0.120647                0.055565           1.598973            2       True         17\n",
      "24   NeuralNetTorch_BAG_L1  -36899.189571 -27163.631306  root_mean_squared_error        0.213151       0.155078  13.088661                 0.213151                0.155078          13.088661            1       True          8\n",
      "25      XGBoost_r33_BAG_L1  -51768.844101 -43505.141361  root_mean_squared_error        0.275234       0.078287   2.096772                 0.275234                0.078287           2.096772            1       True         14\n",
      "26      CatBoost_r9_BAG_L1  -70364.306559 -57579.946356  root_mean_squared_error        0.085724       0.080937   4.098160                 0.085724                0.080937           4.098160            1       True         12\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t263s\t = DyStack   runtime |\t737s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 737s\n",
      "AutoGluon will save models to \"c:\\Users\\Victor\\Desktop\\Kaggle\\house-prices-advanced-regression-techniques\\AutogluonModels\\ag-20250928_093925\"\n",
      "Train Data Rows:    1460\n",
      "Train Data Columns: 77\n",
      "Label Column:       SalePrice\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    11517.96 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.25 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 10 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 17 | ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', ...]\n",
      "\t\t('int', [])    : 38 | ['MSSubClass', 'OverallQual', 'OverallCond', 'ExterQual', 'BsmtQual', ...]\n",
      "\t\t('object', []) : 22 | ['MSZoning', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 21 | ['MSZoning', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', ...]\n",
      "\t\t('float', [])     : 17 | ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', ...]\n",
      "\t\t('int', [])       : 29 | ['MSSubClass', 'OverallQual', 'OverallCond', 'ExterQual', 'BsmtQual', ...]\n",
      "\t\t('int', ['bool']) : 10 | ['CentralAir', 'HasBsmtFinSF2', 'HasBsmtFinSF1', 'Has2ndFlr', 'HasLowQualFinSF', ...]\n",
      "\t0.2s = Fit runtime\n",
      "\t77 features in original data used to generate 77 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.57 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.21s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 491.32s of the 737.15s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.18%)\n",
      "\t-25652.0524\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.1s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 480.99s of the 726.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.18%)\n",
      "\t-26942.1468\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.87s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 476.84s of the 722.67s of remaining time.\n",
      "\t-28865.2981\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.82s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 475.73s of the 721.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=4.19%)\n",
      "\t-25241.479\t = Validation score   (-root_mean_squared_error)\n",
      "\t48.45s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 424.88s of the 670.72s of remaining time.\n",
      "\t-29476.5676\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 424.06s of the 669.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.05%)\n",
      "\t-32005.0174\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.52s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 410.29s of the 656.13s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.33%)\n",
      "\t-26523.802\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.34s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 402.07s of the 647.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\t-27177.8255\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.05s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 380.54s of the 626.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.35%)\n",
      "\t-26071.2362\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.82s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 352.13s of the 597.96s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=2.16%)\n",
      "\t-24840.8589\t = Validation score   (-root_mean_squared_error)\n",
      "\t35.93s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 314.11s of the 559.95s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.02%)\n",
      "\t-29149.3937\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.88s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 293.11s of the 538.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.16%)\n",
      "\t-25837.842\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.41s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 288.46s of the 534.30s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\t-32108.1058\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.6s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 272.61s of the 518.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=2.72%)\n",
      "\t-27004.5493\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.97s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 242.37s of the 488.20s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.06%)\n",
      "\t-26311.1588\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.27s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 232.77s of the 478.61s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.02%)\n",
      "\t-28783.5783\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.6s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 210.05s of the 455.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=1.15%)\n",
      "\t-26548.7191\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.96s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 185.78s of the 431.62s of remaining time.\n",
      "\t-29741.3932\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 185.12s of the 430.96s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=1.88%)\n",
      "\t-26450.4154\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.23s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 163.86s of the 409.70s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\t-30631.9621\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.18s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 143.34s of the 389.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=2.61%)\n",
      "\t-29936.3025\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.01s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 125.88s of the 371.72s of remaining time.\n",
      "\t-28737.238\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.77s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 124.84s of the 370.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.35%)\n",
      "\t-25973.485\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.08s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 114.15s of the 359.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\t-32672.494\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.93s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 98.09s of the 343.93s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.19%)\n",
      "\t-25635.5735\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.95s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 91.51s of the 337.34s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.02%)\n",
      "\t-28830.3856\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.9s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 76.46s of the 322.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.27%)\n",
      "\t-25883.049\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.83s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 64.78s of the 310.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.02%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_r86_BAG_L1.\n",
      "Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 53.50s of the 299.34s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=2.06%)\n",
      "\t-27065.2563\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.98s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 45.30s of the 291.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_r11_BAG_L1.\n",
      "Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 37.98s of the 283.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.34%)\n",
      "\t-25311.9513\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.5s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 31.34s of the 277.18s of remaining time.\n",
      "\t-30690.7147\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.5s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 30.65s of the 276.48s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=2.12%)\n",
      "\t-29242.0013\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.69s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r103_BAG_L1 ... Training model for up to 24.84s of the 270.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_r103_BAG_L1.\n",
      "Fitting model: NeuralNetTorch_r14_BAG_L1 ... Training model for up to 17.53s of the 263.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.02%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_r14_BAG_L1.\n",
      "Fitting model: LightGBM_r161_BAG_L1 ... Training model for up to 10.62s of the 256.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.72%)\n",
      "\t-26041.3684\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.15s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r143_BAG_L1 ... Training model for up to 6.06s of the 251.90s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_r143_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 244.42s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost_r194_BAG_L1': 0.421, 'LightGBMXT_BAG_L1': 0.158, 'NeuralNetTorch_BAG_L1': 0.158, 'CatBoost_r177_BAG_L1': 0.158, 'NeuralNetTorch_r22_BAG_L1': 0.053, 'NeuralNetFastAI_r102_BAG_L1': 0.053}\n",
      "\t-23497.9362\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 244.39s of the 244.30s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.14%)\n",
      "\t-25844.785\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.31s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 230.29s of the 230.21s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.14%)\n",
      "\t-25807.1112\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.74s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 226.37s of the 226.28s of remaining time.\n",
      "\t-25488.7091\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.24s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 224.89s of the 224.81s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=2.44%)\n",
      "\t-25128.9438\t = Validation score   (-root_mean_squared_error)\n",
      "\t23.03s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 199.65s of the 199.56s of remaining time.\n",
      "\t-24840.1329\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 198.79s of the 198.70s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.04%)\n",
      "\t-28257.1257\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.1s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 185.52s of the 185.44s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.28%)\n",
      "\t-24211.6326\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.58s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 179.34s of the 179.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.02%)\n",
      "\t-26535.0753\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.44s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 160.58s of the 160.50s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.51%)\n",
      "\t-26775.4439\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.91s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 152.24s of the 152.15s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=2.46%)\n",
      "\t-25506.997\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.96s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 134.23s of the 134.15s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.02%)\n",
      "\t-28446.237\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.39s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 116.67s of the 116.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t-25304.6545\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.46s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 111.99s of the 111.90s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.04%)\n",
      "\t-29738.409\t = Validation score   (-root_mean_squared_error)\n",
      "\t13.63s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 96.10s of the 96.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=3.24%)\n",
      "\t-33973.452\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.24s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L2 ... Training model for up to 83.58s of the 83.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.09%)\n",
      "\t-26678.8152\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.97s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L2 ... Training model for up to 73.35s of the 73.26s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.02%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_r22_BAG_L2.\n",
      "Fitting model: XGBoost_r33_BAG_L2 ... Training model for up to 61.65s of the 61.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=1.92%)\n",
      "\t-25136.0935\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.0s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L2 ... Training model for up to 52.59s of the 52.51s of remaining time.\n",
      "\t-24593.5194\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L2 ... Training model for up to 51.80s of the 51.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=2.53%)\n",
      "\t-25172.8802\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.91s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L2 ... Training model for up to 43.67s of the 43.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.05%)\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_r102_BAG_L2.\n",
      "Fitting model: CatBoost_r13_BAG_L2 ... Training model for up to 36.49s of the 36.40s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=3.33%)\n",
      "\t-55247.0268\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.27s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForest_r195_BAG_L2 ... Training model for up to 30.15s of the 30.06s of remaining time.\n",
      "\t-25629.6184\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.06s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L2 ... Training model for up to 28.79s of the 28.70s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.55%)\n",
      "\t-26574.5793\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.69s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L2 ... Training model for up to 22.64s of the 22.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.05%)\n",
      "\tTime limit exceeded... Skipping NeuralNetFastAI_r145_BAG_L2.\n",
      "Fitting model: XGBoost_r89_BAG_L2 ... Training model for up to 15.83s of the 15.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.28%)\n",
      "\t-24855.7606\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.36s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L2 ... Training model for up to 11.34s of the 11.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_r30_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 3.97s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost_BAG_L2': 0.524, 'LightGBMXT_BAG_L1': 0.238, 'NeuralNetTorch_BAG_L1': 0.095, 'CatBoost_r177_BAG_L1': 0.095, 'NeuralNetTorch_BAG_L2': 0.048}\n",
      "\t-23446.7705\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 733.43s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 109.4 rows/s (183 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\Victor\\Desktop\\Kaggle\\house-prices-advanced-regression-techniques\\AutogluonModels\\ag-20250928_093925\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          model     score_val              eval_metric  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0           WeightedEnsemble_L3 -23446.770467  root_mean_squared_error       2.267843  194.351807                0.000000           0.013003            3       True         56\n",
      "1           WeightedEnsemble_L2 -23497.936190  root_mean_squared_error       0.932161  105.373841                0.000000           0.014003            2       True         33\n",
      "2                XGBoost_BAG_L2 -24211.632594  root_mean_squared_error       2.094514  177.896679                0.075707           3.580279            2       True         40\n",
      "3         ExtraTrees_r42_BAG_L2 -24593.519415  root_mean_squared_error       2.165391  174.881223                0.146584           0.564824            2       True         50\n",
      "4          ExtraTreesMSE_BAG_L2 -24840.132868  root_mean_squared_error       2.189027  174.922446                0.170221           0.606046            2       True         38\n",
      "5          CatBoost_r177_BAG_L1 -24840.858930  root_mean_squared_error       0.080982   35.926443                0.080982          35.926443            1       True         10\n",
      "6            XGBoost_r89_BAG_L2 -24855.760553  root_mean_squared_error       2.085113  176.674826                0.066306           2.358426            2       True         55\n",
      "7               CatBoost_BAG_L2 -25128.943839  root_mean_squared_error       2.117393  197.348311                0.098587          23.031911            2       True         37\n",
      "8            XGBoost_r33_BAG_L2 -25136.093498  root_mean_squared_error       2.174110  181.313187                0.155303           6.996788            2       True         49\n",
      "9          CatBoost_r137_BAG_L2 -25172.880192  root_mean_squared_error       2.114187  180.222262                0.095380           5.905862            2       True         51\n",
      "10              CatBoost_BAG_L1 -25241.478999  root_mean_squared_error       0.089238   48.451382                0.089238          48.451382            1       True          4\n",
      "11         LightGBM_r131_BAG_L2 -25304.654541  root_mean_squared_error       2.093966  176.777563                0.075159           2.461164            2       True         45\n",
      "12          XGBoost_r194_BAG_L1 -25311.951283  root_mean_squared_error       0.093558    4.498904                0.093558           4.498904            1       True         29\n",
      "13       RandomForestMSE_BAG_L2 -25488.709085  root_mean_squared_error       2.174872  175.555815                0.156066           1.239416            2       True         36\n",
      "14         CatBoost_r177_BAG_L2 -25506.997033  root_mean_squared_error       2.091694  190.274021                0.072888          15.957622            2       True         43\n",
      "15     RandomForest_r195_BAG_L2 -25629.618363  root_mean_squared_error       2.211666  175.373742                0.192860           1.057342            2       True         53\n",
      "16           XGBoost_r89_BAG_L1 -25635.573485  root_mean_squared_error       0.081520    3.946845                0.081520           3.946845            1       True         25\n",
      "17            LightGBMXT_BAG_L1 -25652.052432  root_mean_squared_error       0.223887    7.098248                0.223887           7.098248            1       True          1\n",
      "18              LightGBM_BAG_L2 -25807.111206  root_mean_squared_error       2.068267  176.051755                0.049461           1.735356            2       True         35\n",
      "19         LightGBM_r131_BAG_L1 -25837.841979  root_mean_squared_error       0.124131    2.406532                0.124131           2.406532            1       True         12\n",
      "20            LightGBMXT_BAG_L2 -25844.785042  root_mean_squared_error       2.233299  184.624095                0.214493          10.307696            2       True         34\n",
      "21         LightGBM_r130_BAG_L1 -25883.048976  root_mean_squared_error       0.255977    7.830903                0.255977           7.830903            1       True         27\n",
      "22         LightGBM_r188_BAG_L1 -25973.485027  root_mean_squared_error       0.244850    7.075802                0.244850           7.075802            1       True         23\n",
      "23         LightGBM_r161_BAG_L1 -26041.368446  root_mean_squared_error       0.160132    2.146661                0.160132           2.146661            1       True         32\n",
      "24         LightGBMLarge_BAG_L1 -26071.236229  root_mean_squared_error       0.289257   23.816665                0.289257          23.816665            1       True          9\n",
      "25          LightGBM_r96_BAG_L1 -26311.158850  root_mean_squared_error       0.497663    6.270881                0.497663           6.270881            1       True         15\n",
      "26         CatBoost_r137_BAG_L1 -26450.415381  root_mean_squared_error       0.073418   19.227588                0.073418          19.227588            1       True         19\n",
      "27               XGBoost_BAG_L1 -26523.801994  root_mean_squared_error       0.092142    5.337919                0.092142           5.337919            1       True          7\n",
      "28        NeuralNetTorch_BAG_L2 -26535.075304  root_mean_squared_error       2.192135  190.758524                0.173329          16.442124            2       True         41\n",
      "29           XGBoost_r33_BAG_L1 -26548.719096  root_mean_squared_error       0.297938   21.956455                0.297938          21.956455            1       True         17\n",
      "30         LightGBM_r188_BAG_L2 -26574.579326  root_mean_squared_error       2.121558  178.007997                0.102751           3.691597            2       True         54\n",
      "31          LightGBM_r96_BAG_L2 -26678.815182  root_mean_squared_error       2.319103  181.282454                0.300296           6.966054            2       True         48\n",
      "32         LightGBMLarge_BAG_L2 -26775.443876  root_mean_squared_error       2.094862  180.225029                0.076056           5.908629            2       True         42\n",
      "33              LightGBM_BAG_L1 -26942.146842  root_mean_squared_error       0.058345    1.865528                0.058345           1.865528            1       True          2\n",
      "34           CatBoost_r9_BAG_L1 -27004.549332  root_mean_squared_error       0.086539   27.971703                0.086539          27.971703            1       True         14\n",
      "35          CatBoost_r50_BAG_L1 -27065.256323  root_mean_squared_error       0.094725    5.981326                0.094725           5.981326            1       True         28\n",
      "36        NeuralNetTorch_BAG_L1 -27177.825463  root_mean_squared_error       0.153500   19.046581                0.153500          19.046581            1       True          8\n",
      "37       NeuralNetFastAI_BAG_L2 -28257.125689  root_mean_squared_error       2.115790  185.416528                0.096983          11.100128            2       True         39\n",
      "38    NeuralNetTorch_r79_BAG_L2 -28446.236985  root_mean_squared_error       2.196174  189.703154                0.177367          15.386754            2       True         44\n",
      "39     RandomForest_r195_BAG_L1 -28737.237996  root_mean_squared_error       0.183031    0.774839                0.183031           0.774839            1       True         22\n",
      "40    NeuralNetTorch_r22_BAG_L1 -28783.578273  root_mean_squared_error       0.136602   20.604698                0.136602          20.604698            1       True         16\n",
      "41    NeuralNetTorch_r30_BAG_L1 -28830.385643  root_mean_squared_error       0.183760   12.904445                0.183760          12.904445            1       True         26\n",
      "42       RandomForestMSE_BAG_L1 -28865.298071  root_mean_squared_error       0.208509    0.816949                0.208509           0.816949            1       True          3\n",
      "43    NeuralNetTorch_r79_BAG_L1 -29149.393722  root_mean_squared_error       0.136750   18.879054                0.136750          18.879054            1       True         11\n",
      "44          CatBoost_r69_BAG_L1 -29242.001260  root_mean_squared_error       0.092093    3.689877                0.092093           3.689877            1       True         31\n",
      "45         ExtraTreesMSE_BAG_L1 -29476.567646  root_mean_squared_error       0.164402    0.573621                0.164402           0.573621            1       True          5\n",
      "46  NeuralNetFastAI_r191_BAG_L2 -29738.408982  root_mean_squared_error       2.136331  187.942914                0.117525          13.626514            2       True         46\n",
      "47        ExtraTrees_r42_BAG_L1 -29741.393192  root_mean_squared_error       0.123647    0.464535                0.123647           0.464535            1       True         18\n",
      "48          CatBoost_r13_BAG_L1 -29936.302497  root_mean_squared_error       0.087227   15.009191                0.087227          15.009191            1       True         21\n",
      "49  NeuralNetFastAI_r102_BAG_L1 -30631.962075  root_mean_squared_error       0.243632   18.184965                0.243632          18.184965            1       True         20\n",
      "50       ExtraTrees_r172_BAG_L1 -30690.714685  root_mean_squared_error       0.144844    0.499993                0.144844           0.499993            1       True         30\n",
      "51       NeuralNetFastAI_BAG_L1 -32005.017402  root_mean_squared_error       0.112168   11.521858                0.112168          11.521858            1       True          6\n",
      "52  NeuralNetFastAI_r191_BAG_L1 -32108.105783  root_mean_squared_error       0.119120   13.604561                0.119120          13.604561            1       True         13\n",
      "53  NeuralNetFastAI_r145_BAG_L1 -32672.493995  root_mean_squared_error       0.154148   13.926710                0.154148          13.926710            1       True         24\n",
      "54           CatBoost_r9_BAG_L2 -33973.452028  root_mean_squared_error       2.099629  184.558725                0.080822          10.242325            2       True         47\n",
      "55          CatBoost_r13_BAG_L2 -55247.026846  root_mean_squared_error       2.110232  178.584639                0.091425           4.268239            2       True         52\n",
      "✅ Fichier submission_autogluon.csv généré !\n"
     ]
    }
   ],
   "source": [
    "test_data = X_test\n",
    "train_data = X_train\n",
    "train_data['SalePrice'] = y_train\n",
    "\n",
    "label = \"SalePrice\"\n",
    "\n",
    "train_ag = TabularDataset(train_data)\n",
    "\n",
    "\n",
    "# Création du prédicteur\n",
    "predictor = TabularPredictor(\n",
    "        label=label,\n",
    "        eval_metric=\"root_mean_squared_error\",\n",
    "        verbosity=2\n",
    "    ).fit(\n",
    "        train_data=train_ag,\n",
    "        time_limit=1000,   # 10 minutes d'entraînement (tu peux augmenter)\n",
    "        presets=\"best_quality\",  # meilleur modèle possible (un peu plus lent)\n",
    "        num_cpus=1,       # évite Ray multi-process\n",
    "        num_gpus=0        # désactive GPU si instable\n",
    "    )\n",
    "\n",
    "    # Leaderboard (tous les modèles testés et leurs scores)\n",
    "predictor.leaderboard(silent=False)\n",
    "\n",
    "# Prédictions sur le test set\n",
    "test_ag = TabularDataset(test_data)\n",
    "\n",
    "preds = predictor.predict(test_ag)\n",
    "\n",
    "# Sauvegarde en CSV au format Kaggle\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"SalePrice\": preds\n",
    "})\n",
    "submission.to_csv(\"submission_autogluon.csv\", index=False)\n",
    "\n",
    "print(\"✅ Fichier submission_autogluon.csv généré !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4aca86",
   "metadata": {},
   "source": [
    "#  Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c255ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-28 11:56:07,279] A new study created in memory with name: no-name-21db261b-5699-48a4-ba8b-bb44911c2c3c\n",
      "C:\\Users\\Victor\\AppData\\Local\\Temp\\ipykernel_25200\\4124765906.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\Victor\\AppData\\Local\\Temp\\ipykernel_25200\\4124765906.py:49: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\Victor\\AppData\\Local\\Temp\\ipykernel_25200\\4124765906.py:51: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
      "C:\\Users\\Victor\\AppData\\Local\\Temp\\ipykernel_25200\\4124765906.py:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
      "C:\\Users\\Victor\\AppData\\Local\\Temp\\ipykernel_25200\\4124765906.py:12: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n",
      "C:\\Users\\Victor\\AppData\\Local\\Temp\\ipykernel_25200\\4124765906.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-8, 10.0),\n",
      "C:\\Users\\Victor\\AppData\\Local\\Temp\\ipykernel_25200\\4124765906.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-8, 10.0),\n",
      "[I 2025-09-28 11:56:14,613] Trial 0 finished with value: 38020.361979166664 and parameters: {'model': 'xgb', 'n_estimators': 1096, 'max_depth': 6, 'learning_rate': 0.0011642753825667589, 'subsample': 0.6468788204107296, 'colsample_bytree': 0.5957564484284796, 'reg_alpha': 2.7463220175185498e-06, 'reg_lambda': 2.8452341932917924e-08}. Best is trial 0 with value: 38020.361979166664.\n",
      "C:\\Users\\Victor\\AppData\\Local\\Temp\\ipykernel_25200\\4124765906.py:24: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "C:\\Users\\Victor\\AppData\\Local\\Temp\\ipykernel_25200\\4124765906.py:25: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
      "C:\\Users\\Victor\\AppData\\Local\\Temp\\ipykernel_25200\\4124765906.py:26: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n",
      "C:\\Users\\Victor\\AppData\\Local\\Temp\\ipykernel_25200\\4124765906.py:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-8, 10.0),\n",
      "C:\\Users\\Victor\\AppData\\Local\\Temp\\ipykernel_25200\\4124765906.py:28: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-8, 10.0),\n",
      "[I 2025-09-28 11:56:17,716] Trial 2 finished with value: 30316.350867650588 and parameters: {'model': 'lgbm', 'n_estimators': 1062, 'num_leaves': 80, 'learning_rate': 0.006069271590324952, 'subsample': 0.7238553857061394, 'colsample_bytree': 0.9959703822836928, 'reg_alpha': 0.010232938623251949, 'reg_lambda': 0.6938025427112972}. Best is trial 2 with value: 30316.350867650588.\n",
      "[I 2025-09-28 11:56:19,389] Trial 3 finished with value: 33878.93185075775 and parameters: {'model': 'lgbm', 'n_estimators': 928, 'num_leaves': 28, 'learning_rate': 0.23677273430533993, 'subsample': 0.732286263143485, 'colsample_bytree': 0.8966733604968053, 'reg_alpha': 6.669720583940124e-08, 'reg_lambda': 2.5229379991472124e-05}. Best is trial 2 with value: 30316.350867650588.\n",
      "[I 2025-09-28 11:56:19,780] Trial 1 finished with value: 33052.49803501658 and parameters: {'model': 'gbr', 'n_estimators': 1139, 'learning_rate': 0.2464986714242124, 'max_depth': 3, 'subsample': 0.9145774886142839, 'min_samples_split': 15, 'min_samples_leaf': 6}. Best is trial 2 with value: 30316.350867650588.\n",
      "[I 2025-09-28 11:56:27,365] Trial 4 finished with value: 40156.51816421908 and parameters: {'model': 'gbr', 'n_estimators': 674, 'learning_rate': 0.001670878306517881, 'max_depth': 8, 'subsample': 0.8491365802062278, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 2 with value: 30316.350867650588.\n",
      "[I 2025-09-28 11:56:32,303] Trial 5 finished with value: 31258.862444696337 and parameters: {'model': 'gbr', 'n_estimators': 688, 'learning_rate': 0.009659895486344069, 'max_depth': 5, 'subsample': 0.8105035394182474, 'min_samples_split': 14, 'min_samples_leaf': 8}. Best is trial 2 with value: 30316.350867650588.\n",
      "[I 2025-09-28 11:56:33,565] Trial 7 finished with value: 28924.55859375 and parameters: {'model': 'xgb', 'n_estimators': 1219, 'max_depth': 5, 'learning_rate': 0.04779787976560815, 'subsample': 0.7950203722833794, 'colsample_bytree': 0.6910515305527534, 'reg_alpha': 4.518443617311852e-05, 'reg_lambda': 3.3018076044063115e-06}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:56:34,656] Trial 8 finished with value: 30777.961580067073 and parameters: {'model': 'lgbm', 'n_estimators': 824, 'num_leaves': 70, 'learning_rate': 0.0040240576989693235, 'subsample': 0.7123998809394192, 'colsample_bytree': 0.95013143910165, 'reg_alpha': 2.201055952094011, 'reg_lambda': 4.24396378213271e-05}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:56:35,110] Trial 6 finished with value: 30181.168472990263 and parameters: {'model': 'gbr', 'n_estimators': 1100, 'learning_rate': 0.08001581845503838, 'max_depth': 4, 'subsample': 0.549616908761459, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:56:42,749] Trial 9 finished with value: 33570.070745401455 and parameters: {'model': 'gbr', 'n_estimators': 1300, 'learning_rate': 0.001419341343912946, 'max_depth': 6, 'subsample': 0.5582810760177125, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:56:51,256] Trial 11 finished with value: 33422.02734375 and parameters: {'model': 'xgb', 'n_estimators': 1479, 'max_depth': 10, 'learning_rate': 0.03552354259256341, 'subsample': 0.982853152247602, 'colsample_bytree': 0.6920321282938595, 'reg_alpha': 0.00020205386664725865, 'reg_lambda': 4.036589996605641e-08}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:56:52,378] Trial 12 finished with value: 30283.3046875 and parameters: {'model': 'xgb', 'n_estimators': 1285, 'max_depth': 3, 'learning_rate': 0.043964405601806415, 'subsample': 0.5488498922766956, 'colsample_bytree': 0.7751197824691555, 'reg_alpha': 0.00021204279318895474, 'reg_lambda': 0.017559585273447954}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:56:52,953] Trial 13 finished with value: 30259.8515625 and parameters: {'model': 'xgb', 'n_estimators': 485, 'max_depth': 4, 'learning_rate': 0.0750637327881172, 'subsample': 0.6263224413597971, 'colsample_bytree': 0.5042761328865604, 'reg_alpha': 0.02000725646324434, 'reg_lambda': 2.5787108668663447e-06}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:56:55,496] Trial 10 finished with value: 31449.634991764993 and parameters: {'model': 'gbr', 'n_estimators': 1427, 'learning_rate': 0.002670374507303131, 'max_depth': 5, 'subsample': 0.8904298658042757, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:57:00,891] Trial 15 finished with value: 31311.119140625 and parameters: {'model': 'xgb', 'n_estimators': 1232, 'max_depth': 8, 'learning_rate': 0.1037686705368067, 'subsample': 0.5008170671401804, 'colsample_bytree': 0.7640897445085114, 'reg_alpha': 1.6039167143539216e-06, 'reg_lambda': 0.0021507286732112734}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:57:02,871] Trial 14 finished with value: 31302.02401209941 and parameters: {'model': 'gbr', 'n_estimators': 1429, 'learning_rate': 0.0964795813858541, 'max_depth': 5, 'subsample': 0.509357177888377, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:57:04,508] Trial 17 finished with value: 29060.536458333332 and parameters: {'model': 'xgb', 'n_estimators': 987, 'max_depth': 4, 'learning_rate': 0.017798020277438594, 'subsample': 0.7893714619530147, 'colsample_bytree': 0.6459035786871484, 'reg_alpha': 1.4003475075688489e-08, 'reg_lambda': 1.0630191753932565e-06}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:57:06,740] Trial 16 finished with value: 29708.05914138696 and parameters: {'model': 'gbr', 'n_estimators': 910, 'learning_rate': 0.02174962557397729, 'max_depth': 4, 'subsample': 0.7804696003021507, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:57:07,779] Trial 19 finished with value: 30067.079427083332 and parameters: {'model': 'xgb', 'n_estimators': 322, 'max_depth': 7, 'learning_rate': 0.01415539857562623, 'subsample': 0.8355411216539081, 'colsample_bytree': 0.6527531479303076, 'reg_alpha': 1.283786262177102e-08, 'reg_lambda': 1.2406548124965545e-06}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:57:08,129] Trial 18 finished with value: 29712.188802083332 and parameters: {'model': 'xgb', 'n_estimators': 913, 'max_depth': 7, 'learning_rate': 0.01567293555636038, 'subsample': 0.7934380007849294, 'colsample_bytree': 0.6571223640635768, 'reg_alpha': 1.4023191421845887e-08, 'reg_lambda': 2.083100877503735e-06}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:57:09,426] Trial 20 finished with value: 29755.549479166668 and parameters: {'model': 'xgb', 'n_estimators': 986, 'max_depth': 7, 'learning_rate': 0.02531529196832938, 'subsample': 0.6660573422850073, 'colsample_bytree': 0.5767924099851607, 'reg_alpha': 2.4099543967884678e-06, 'reg_lambda': 6.215673099208006e-07}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:57:09,587] Trial 21 finished with value: 30046.093098958332 and parameters: {'model': 'xgb', 'n_estimators': 763, 'max_depth': 3, 'learning_rate': 0.02879708736898386, 'subsample': 0.6667605762791488, 'colsample_bytree': 0.8192161711402436, 'reg_alpha': 2.341786446870604e-06, 'reg_lambda': 0.00022556389705010037}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:57:10,041] Trial 22 finished with value: 28943.499348958332 and parameters: {'model': 'xgb', 'n_estimators': 756, 'max_depth': 4, 'learning_rate': 0.025330780555060745, 'subsample': 0.782989148632093, 'colsample_bytree': 0.8440442812348047, 'reg_alpha': 2.0214464396148296e-05, 'reg_lambda': 0.00027102842896901204}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:57:10,912] Trial 23 finished with value: 29031.421223958332 and parameters: {'model': 'xgb', 'n_estimators': 1191, 'max_depth': 4, 'learning_rate': 0.010042981783955107, 'subsample': 0.7722927927136494, 'colsample_bytree': 0.7091560089005315, 'reg_alpha': 0.005969624532977547, 'reg_lambda': 1.1886547269440804e-08}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:57:10,959] Trial 24 finished with value: 29612.171875 and parameters: {'model': 'xgb', 'n_estimators': 568, 'max_depth': 4, 'learning_rate': 0.04834853602234196, 'subsample': 0.876597108158208, 'colsample_bytree': 0.7180149218699009, 'reg_alpha': 3.7973484896431923e-05, 'reg_lambda': 0.003704663388748071}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:57:11,613] Trial 25 finished with value: 29444.337239583332 and parameters: {'model': 'xgb', 'n_estimators': 550, 'max_depth': 5, 'learning_rate': 0.05232404015850848, 'subsample': 0.8798991614742313, 'colsample_bytree': 0.8315549145986948, 'reg_alpha': 0.00217222224892851, 'reg_lambda': 0.0033457266377236988}. Best is trial 7 with value: 28924.55859375.\n",
      "[I 2025-09-28 11:57:12,978] Trial 26 finished with value: 28914.921875 and parameters: {'model': 'xgb', 'n_estimators': 1206, 'max_depth': 5, 'learning_rate': 0.009394310919811978, 'subsample': 0.7576599777941764, 'colsample_bytree': 0.8385948672199303, 'reg_alpha': 0.006640753861216473, 'reg_lambda': 1.0044433139533467e-08}. Best is trial 26 with value: 28914.921875.\n",
      "[I 2025-09-28 11:57:14,814] Trial 27 finished with value: 29183.417317708332 and parameters: {'model': 'xgb', 'n_estimators': 1254, 'max_depth': 6, 'learning_rate': 0.009086030002333139, 'subsample': 0.7543217511567895, 'colsample_bytree': 0.8570307915308878, 'reg_alpha': 0.40769091166550075, 'reg_lambda': 1.292130000125983e-07}. Best is trial 26 with value: 28914.921875.\n",
      "[I 2025-09-28 11:57:15,724] Trial 28 finished with value: 30514.278586742774 and parameters: {'model': 'lgbm', 'n_estimators': 1358, 'num_leaves': 23, 'learning_rate': 0.00814310092445136, 'subsample': 0.9465893649000277, 'colsample_bytree': 0.859579503830568, 'reg_alpha': 0.144274657550696, 'reg_lambda': 2.6301926399276282e-05}. Best is trial 26 with value: 28914.921875.\n",
      "[I 2025-09-28 11:57:16,382] Trial 29 finished with value: 30016.77634802546 and parameters: {'model': 'lgbm', 'n_estimators': 1368, 'num_leaves': 20, 'learning_rate': 0.005597865390292237, 'subsample': 0.9434919250772026, 'colsample_bytree': 0.8892053281139809, 'reg_alpha': 2.96743786700068e-05, 'reg_lambda': 2.0451320662160115e-05}. Best is trial 26 with value: 28914.921875.\n",
      "[I 2025-09-28 11:57:16,956] Trial 30 finished with value: 30495.130208333332 and parameters: {'model': 'xgb', 'n_estimators': 780, 'max_depth': 5, 'learning_rate': 0.150057353191126, 'subsample': 0.700032364657686, 'colsample_bytree': 0.7883665267743241, 'reg_alpha': 1.8232164845771996e-05, 'reg_lambda': 0.5393660141758191}. Best is trial 26 with value: 28914.921875.\n",
      "[I 2025-09-28 11:57:17,854] Trial 31 finished with value: 31613.008463541668 and parameters: {'model': 'xgb', 'n_estimators': 1029, 'max_depth': 5, 'learning_rate': 0.1591184713560278, 'subsample': 0.7007157365031135, 'colsample_bytree': 0.7919294981792168, 'reg_alpha': 0.0008656989265498395, 'reg_lambda': 0.15486421349627133}. Best is trial 26 with value: 28914.921875.\n",
      "[I 2025-09-28 11:57:18,658] Trial 32 finished with value: 28723.017578125 and parameters: {'model': 'xgb', 'n_estimators': 1192, 'max_depth': 4, 'learning_rate': 0.013984560043503194, 'subsample': 0.7597026159170419, 'colsample_bytree': 0.7127354971624859, 'reg_alpha': 0.004623060774439501, 'reg_lambda': 1.9191450763197594e-08}. Best is trial 32 with value: 28723.017578125.\n",
      "[I 2025-09-28 11:57:19,255] Trial 33 finished with value: 28966.9921875 and parameters: {'model': 'xgb', 'n_estimators': 1164, 'max_depth': 4, 'learning_rate': 0.01316276094829572, 'subsample': 0.7559230823791415, 'colsample_bytree': 0.726680058237068, 'reg_alpha': 0.006829471414297921, 'reg_lambda': 2.0305479999232955e-08}. Best is trial 32 with value: 28723.017578125.\n",
      "[I 2025-09-28 11:57:19,386] Trial 34 finished with value: 29379.843098958332 and parameters: {'model': 'xgb', 'n_estimators': 1141, 'max_depth': 3, 'learning_rate': 0.014835538207394031, 'subsample': 0.8241736624529927, 'colsample_bytree': 0.7313293842174109, 'reg_alpha': 0.05413472364954892, 'reg_lambda': 1.966936460844924e-07}. Best is trial 32 with value: 28723.017578125.\n",
      "[I 2025-09-28 11:57:19,972] Trial 35 finished with value: 29335.8828125 and parameters: {'model': 'xgb', 'n_estimators': 1120, 'max_depth': 3, 'learning_rate': 0.03399524901037561, 'subsample': 0.8239491708895472, 'colsample_bytree': 0.9199902622486035, 'reg_alpha': 0.0722504979873577, 'reg_lambda': 1.3860966566543085e-07}. Best is trial 32 with value: 28723.017578125.\n",
      "[I 2025-09-28 11:57:21,451] Trial 36 finished with value: 29883.090494791668 and parameters: {'model': 'xgb', 'n_estimators': 845, 'max_depth': 6, 'learning_rate': 0.005517128620376454, 'subsample': 0.8538534112846634, 'colsample_bytree': 0.9103200874578005, 'reg_alpha': 0.0009823430224355468, 'reg_lambda': 1.280928129945066e-07}. Best is trial 32 with value: 28723.017578125.\n",
      "[I 2025-09-28 11:57:22,978] Trial 37 finished with value: 29183.662109375 and parameters: {'model': 'xgb', 'n_estimators': 1054, 'max_depth': 6, 'learning_rate': 0.005836541414173773, 'subsample': 0.7300520973731596, 'colsample_bytree': 0.8154253241572921, 'reg_alpha': 0.000980725293858162, 'reg_lambda': 0.00013678774577096801}. Best is trial 32 with value: 28723.017578125.\n",
      "[I 2025-09-28 11:57:23,731] Trial 38 finished with value: 29961.152238961873 and parameters: {'model': 'lgbm', 'n_estimators': 1030, 'num_leaves': 100, 'learning_rate': 0.0038212216714488233, 'subsample': 0.746130913935124, 'colsample_bytree': 0.6742346931697862, 'reg_alpha': 0.00011132226399790194, 'reg_lambda': 0.0004500305261801288}. Best is trial 32 with value: 28723.017578125.\n",
      "[I 2025-09-28 11:57:24,825] Trial 39 finished with value: 29585.076169847107 and parameters: {'model': 'lgbm', 'n_estimators': 1333, 'num_leaves': 97, 'learning_rate': 0.003515369943052201, 'subsample': 0.6190664958212028, 'colsample_bytree': 0.5975637315323165, 'reg_alpha': 3.8133509614482913e-07, 'reg_lambda': 9.026010202039471e-06}. Best is trial 32 with value: 28723.017578125.\n",
      "[I 2025-09-28 11:57:25,971] Trial 40 finished with value: 29271.290364583332 and parameters: {'model': 'xgb', 'n_estimators': 1222, 'max_depth': 5, 'learning_rate': 0.020134881914166097, 'subsample': 0.6149753868445603, 'colsample_bytree': 0.6057641876928649, 'reg_alpha': 3.808979951531545e-07, 'reg_lambda': 7.6063739225903495e-06}. Best is trial 32 with value: 28723.017578125.\n",
      "[I 2025-09-28 11:57:26,207] Trial 41 finished with value: 28667.032552083332 and parameters: {'model': 'xgb', 'n_estimators': 693, 'max_depth': 5, 'learning_rate': 0.022667426157108015, 'subsample': 0.8076080163549024, 'colsample_bytree': 0.6096128397576875, 'reg_alpha': 1.1719534986465034e-05, 'reg_lambda': 5.332035292141814e-08}. Best is trial 41 with value: 28667.032552083332.\n",
      "[I 2025-09-28 11:57:26,917] Trial 42 finished with value: 29041.671875 and parameters: {'model': 'xgb', 'n_estimators': 1181, 'max_depth': 4, 'learning_rate': 0.01178281323186331, 'subsample': 0.7592417494744882, 'colsample_bytree': 0.7501236024962527, 'reg_alpha': 0.005574107524358513, 'reg_lambda': 2.0754514793543667e-08}. Best is trial 41 with value: 28667.032552083332.\n",
      "[I 2025-09-28 11:57:27,587] Trial 43 finished with value: 30116.968098958332 and parameters: {'model': 'xgb', 'n_estimators': 670, 'max_depth': 5, 'learning_rate': 0.05884433246365814, 'subsample': 0.809433934251862, 'colsample_bytree': 0.5655002032203755, 'reg_alpha': 8.897110096787327e-06, 'reg_lambda': 3.926942482092507e-08}. Best is trial 41 with value: 28667.032552083332.\n",
      "[I 2025-09-28 11:57:28,162] Trial 44 finished with value: 29917.166015625 and parameters: {'model': 'xgb', 'n_estimators': 648, 'max_depth': 5, 'learning_rate': 0.06066247789645806, 'subsample': 0.8128526511627077, 'colsample_bytree': 0.5518922040156778, 'reg_alpha': 9.695897528956742e-06, 'reg_lambda': 4.903854330054109e-08}. Best is trial 41 with value: 28667.032552083332.\n",
      "[I 2025-09-28 11:57:28,637] Trial 45 finished with value: 29837.841796875 and parameters: {'model': 'xgb', 'n_estimators': 710, 'max_depth': 6, 'learning_rate': 0.03557795648750699, 'subsample': 0.855489380517489, 'colsample_bytree': 0.6899704993256663, 'reg_alpha': 6.848808266382075e-05, 'reg_lambda': 5.04725222634437e-07}. Best is trial 41 with value: 28667.032552083332.\n",
      "[I 2025-09-28 11:57:29,488] Trial 46 finished with value: 29780.253255208332 and parameters: {'model': 'xgb', 'n_estimators': 742, 'max_depth': 6, 'learning_rate': 0.032660163029700644, 'subsample': 0.8567223610768404, 'colsample_bytree': 0.627873216268948, 'reg_alpha': 4.214229118976277e-05, 'reg_lambda': 2.618456840464917e-07}. Best is trial 41 with value: 28667.032552083332.\n",
      "[I 2025-09-28 11:57:33,609] Trial 47 finished with value: 29993.460097114992 and parameters: {'model': 'gbr', 'n_estimators': 807, 'learning_rate': 0.007362865170422914, 'max_depth': 4, 'subsample': 0.6860690649482253, 'min_samples_split': 19, 'min_samples_leaf': 10}. Best is trial 41 with value: 28667.032552083332.\n",
      "[I 2025-09-28 11:57:36,603] Trial 48 finished with value: 30066.96158809295 and parameters: {'model': 'gbr', 'n_estimators': 601, 'learning_rate': 0.007168676181532482, 'max_depth': 4, 'subsample': 0.7202021622545483, 'min_samples_split': 18, 'min_samples_leaf': 10}. Best is trial 41 with value: 28667.032552083332.\n",
      "[I 2025-09-28 11:57:37,007] Trial 49 finished with value: 30587.640625 and parameters: {'model': 'xgb', 'n_estimators': 573, 'max_depth': 10, 'learning_rate': 0.023880542891885757, 'subsample': 0.7301836019098152, 'colsample_bytree': 0.5343130122077524, 'reg_alpha': 0.0003427945555304724, 'reg_lambda': 4.815478501000108e-08}. Best is trial 41 with value: 28667.032552083332.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  RMSE: 28667.032552083332\n",
      "  Params: {'model': 'xgb', 'n_estimators': 693, 'max_depth': 5, 'learning_rate': 0.022667426157108015, 'subsample': 0.8076080163549024, 'colsample_bytree': 0.6096128397576875, 'reg_alpha': 1.1719534986465034e-05, 'reg_lambda': 5.332035292141814e-08}\n",
      "Submission saved as submission_optuna.csv\n"
     ]
    }
   ],
   "source": [
    "if True:  # mettre à True pour activer l'optimisation\n",
    "    def objective(trial):\n",
    "        # Choix du modèle\n",
    "        model_name = trial.suggest_categorical(\"model\", [\"xgb\", \"lgbm\", \"gbr\"])\n",
    "        \n",
    "        if model_name == \"xgb\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1500),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
    "                \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "                \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-8, 10.0),\n",
    "                \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-8, 10.0),\n",
    "                \"random_state\": 42,\n",
    "                \"n_jobs\": 4,\n",
    "            }\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "        \n",
    "        elif model_name == \"lgbm\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1500),\n",
    "                \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 100),\n",
    "                \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
    "                \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "                \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-8, 10.0),\n",
    "                \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-8, 10.0),\n",
    "                \"random_state\": 42,\n",
    "                \"n_jobs\": 4\n",
    "            }\n",
    "            model = lgb.LGBMRegressor(**params)\n",
    "        \n",
    "        elif model_name == \"rf\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 2000),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 5, 50),\n",
    "                \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "                \"random_state\": 42,\n",
    "                \"n_jobs\": 4\n",
    "            }\n",
    "            model = RandomForestRegressor(**params)\n",
    "        \n",
    "        elif model_name == \"gbr\":\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1500),\n",
    "                \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
    "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
    "                \"random_state\": 42\n",
    "            }\n",
    "            model = GradientBoostingRegressor(**params)\n",
    "        \n",
    "        # Pipeline avec preprocessing\n",
    "        pipeline = Pipeline(steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"model\", model)\n",
    "        ])\n",
    "        \n",
    "        # Validation croisée\n",
    "        cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        scores = cross_val_score(\n",
    "            pipeline, X_train, y_train,\n",
    "            cv=cv, scoring=\"neg_root_mean_squared_error\", n_jobs=4\n",
    "        )\n",
    "        \n",
    "        rmse = -scores.mean()\n",
    "        return rmse\n",
    "\n",
    "\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=1)\n",
    "        )  # on veut minimiser le RMSE\n",
    "    study.optimize(objective, n_trials=500, n_jobs=2)  # 50 essais, en parallèle\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    print(\"  RMSE:\", study.best_value)\n",
    "    print(\"  Params:\", study.best_params)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_model_name = best_params[\"model\"]\n",
    "\n",
    "    # enlever la clé \"model\"\n",
    "    model_params = {k: v for k, v in best_params.items() if k != \"model\"}\n",
    "\n",
    "    if best_model_name == \"xgb\":\n",
    "        model = xgb.XGBRegressor(**model_params)\n",
    "    elif best_model_name == \"lgbm\":\n",
    "        model = lgb.LGBMRegressor(**model_params)\n",
    "    elif best_model_name == \"rf\":\n",
    "        model = RandomForestRegressor(**model_params)\n",
    "    elif best_model_name == \"gbr\":\n",
    "        model = GradientBoostingRegressor(**model_params)\n",
    "\n",
    "    best_model = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    # fit + prédiction\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        \"Id\": test_ids,  \n",
    "        \"SalePrice\": y_pred\n",
    "    })\n",
    "    submission.to_csv(\"submission_optuna.csv\", index=False)\n",
    "\n",
    "    print(\"Submission saved as submission_optuna.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
